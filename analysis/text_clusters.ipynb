{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from collections import defaultdict\n",
    "import json \n",
    "import pathlib \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open(\"/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/dev_from_mturk/annotations.json\"))['annotations']\n",
    "questions = json.load(open(\"/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/dev_from_mturk/questions.json\"))['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clusters from annotations \n",
    "def get_annotator_clusters(questions, annotations): \n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters_by_qid = {}\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        clusters = defaultdict(list)\n",
    "        for quest, ann in list_of_qas:\n",
    "            rewritten = quest['new_question']\n",
    "            answer = ann['answers'][0]['answer']\n",
    "            answer_id = ann['answers'][0]['mturk_id']\n",
    "            cluster_dict = {\"answer\": answer, \"id\": answer_id} \n",
    "            clusters[rewritten].append(cluster_dict)\n",
    "        clusters_by_qid[qid] = clusters\n",
    "    return clusters_by_qid\n",
    "\n",
    "# get the clusters from kmeans preprocessing\n",
    "def get_preprocessed_clusters(questions, annotations): \n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters_by_qid = {}\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        clusters = defaultdict(list)\n",
    "        for quest, ann in list_of_qas:\n",
    "            answer = ann['answers'][0]['answer']\n",
    "            answer_id = ann['answers'][0]['mturk_id']\n",
    "            id_key, answer_id_suffix = answer_id.split(\".\")\n",
    "            cluster_dict = {\"answer\": answer, \"id\": answer_id} \n",
    "            clusters[id_key].append(cluster_dict)\n",
    "        clusters_by_qid[qid] = clusters\n",
    "    return clusters_by_qid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_path = pathlib.Path('').resolve().parent\n",
    "sys.path.insert(0, str(curr_path.joinpath(\"hit3.0\").joinpath(\"results\")))\n",
    "from process_csv import f1_score\n",
    "\n",
    "def preprocess(cluster_data):\n",
    "    if type(cluster_data) in [dict, defaultdict]:\n",
    "        # dealing with predicted clusters or preprocessed clusters\n",
    "        return cluster_data.values()\n",
    "    return cluster_data\n",
    "\n",
    "def get_scores(clusters_by_qid_a, clusters_by_qid_b):\n",
    "    scores = []\n",
    "    for qid in clusters_by_qid_a.keys():\n",
    "        cluster_a = preprocess(clusters_by_qid_a[qid])\n",
    "        cluster_b = preprocess(clusters_by_qid_b[qid])\n",
    "        f1_tuple = f1_score(cluster_a, cluster_b)\n",
    "        scores.append(f1_tuple)\n",
    "    # print(scores)\n",
    "    scores = np.array(scores)\n",
    "    return np.mean(scores, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BARTScore.abstract_class import BertSimilarityScore\n",
    "score_cls = BertSimilarityScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import scipy \n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def read_generations(output_path):\n",
    "    flat_data_by_qid = {}\n",
    "    data = open(output_path).readlines()\n",
    "    for line in data:\n",
    "        batch_data = json.loads(line)\n",
    "        for qid, generation in zip(batch_data['question_id'], batch_data['speaker_utterances'][0]):\n",
    "            flat_data_by_qid[qid] = generation\n",
    "    return flat_data_by_qid\n",
    "\n",
    "def clean_text(text): \n",
    "    text = re.sub(\"<.*?>\", \"\", text)\n",
    "    text = text.strip() \n",
    "    return text \n",
    "\n",
    "# get the clusters from predictions \n",
    "def get_prediction_clusters(predictions_jsonl,\n",
    "                            questions, \n",
    "                            annotations, \n",
    "                            score_cls, \n",
    "                            t = 1.06, \n",
    "                            criterion = \"centroid\", \n",
    "                            method = \"distance\"):\n",
    "    generations_by_qid = read_generations(predictions_jsonl)\n",
    "    anns_by_qid = defaultdict(list)\n",
    "    answers_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        generation = clean_text(generations_by_qid[quest['question_id']])\n",
    "        anns_by_qid[qid].append(generation)\n",
    "        answers_by_qid[qid].append(ann['answers'])\n",
    "\n",
    "    scores_by_qid = {} \n",
    "    clusts_by_qid = {}\n",
    "    # Get matrix of scores \n",
    "    answer_clusters = {}\n",
    "    for qid, quest_list in tqdm(anns_by_qid.items()): \n",
    "        scores = np.zeros((len(quest_list), len(quest_list))) \n",
    "        done = []\n",
    "        for i, q1 in enumerate(quest_list): \n",
    "            for j, q2 in enumerate(quest_list):\n",
    "                if i == j: \n",
    "                    scores[i,j] = 0.0 \n",
    "                    continue\n",
    "                sim_score = score_cls.get_similarity(q1, q2) \n",
    "                scores[i,j] = 1/sim_score\n",
    "                done.append((i,j))\n",
    "                done.append((j,i))\n",
    "\n",
    "        scores_by_qid[qid] = scores \n",
    "        scores = scipy.spatial.distance.squareform(scores)\n",
    "        link = linkage(scores, method=method, metric=\"cosine\")\n",
    "        clust = fcluster(link, t=t, criterion=criterion)\n",
    "\n",
    "        clusts_by_qid[qid] = clust \n",
    "        answers_clustered = defaultdict(list)\n",
    "        ans_list = answers_by_qid[qid]\n",
    "        for i, idx in enumerate(clust):\n",
    "            answer = ans_list[i]\n",
    "            orig_id = answer[0]['mturk_id']\n",
    "            cluster_dict = {\"answer\": answer[0]['answer'], \"id\": orig_id} \n",
    "            answers_clustered[f\"g{idx}\"].append(cluster_dict)\n",
    "        answer_clusters[qid] = answers_clustered\n",
    "\n",
    "    return answer_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity(50)\n",
    "# logger = logging.get_logger(\"transformers\")\n",
    "# logger.info(\"INFO\")\n",
    "# logger.warning(\"WARN\")\n",
    "\n",
    "pred_path = \"/brtx/602-nvme1/estengel/annotator_uncertainty/models/img2q_t5_small/output/predictions_forced.jsonl\"\n",
    "pred_clusters = get_prediction_clusters(pred_path,\n",
    "                                        questions, \n",
    "                                        annotations,\n",
    "                                        score_cls=score_cls,\n",
    "                                        criterion=\"distance\",\n",
    "                                        method=\"centroid\",\n",
    "                                        t=1.06)\n",
    "\n",
    "ann_clusters = get_annotator_clusters(questions, annotations)\n",
    "\n",
    "pred_to_ann = get_scores(pred_clusters, ann_clusters)\n",
    "print(f\"P: {pred_to_ann[1]*100:.2f}, R: {pred_to_ann[2]*100:.2f}, F1: {pred_to_ann[0]*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in [\"centroid\"]:\n",
    "    # for crit in ['inconsistent', 'distance']: \n",
    "    for crit in ['distance']:\n",
    "        if crit == \"distance\":\n",
    "            t_choices = [1.00, 1.01, 1.02, 1.03,  1.04,  1.05,  1.06, 1.07, 0.08, 1.09, 1.10] \n",
    "        else:\n",
    "            t_choices = [0.0, 1.0, 2.0, 3.0]\n",
    "        for t in t_choices:\n",
    "            pred_clusters = get_prediction_clusters(pred_path,\n",
    "                                            questions, \n",
    "                                            annotations,\n",
    "                                            score_cls=score_cls,\n",
    "                                            criterion=crit,\n",
    "                                            method = method,\n",
    "                                            t=t)\n",
    "\n",
    "            pred_to_ann = get_scores(pred_clusters, ann_clusters)\n",
    "            print(f\"Method: {method}, Crit: {crit}, t: {t}, P: {pred_to_ann[1]*100:.2f}, R: {pred_to_ann[2]*100:.2f}, F1: {pred_to_ann[0]*100:.2f}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST: Method: centroid, Crit: distance, t: 1.06, P: 69.21, R: 96.66, F1: 77.83"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a71203a0affc7207a6dfd2214cf786ae35ec96fb0be57392a936686760a696bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
