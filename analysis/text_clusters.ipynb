{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from collections import defaultdict\n",
    "import json \n",
    "import pathlib \n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json.load(open(\"/home/estengel/annotator_uncertainty/jimena_work/cleaned_data/csv/dev_set/annotations.json\"))['annotations']\n",
    "questions = json.load(open(\"/home/estengel/annotator_uncertainty/jimena_work/cleaned_data/csv/dev_set/questions.json\"))['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clusters from annotations \n",
    "def get_annotator_clusters(questions, annotations): \n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters_by_qid = {}\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        clusters = defaultdict(list)\n",
    "        for quest, ann in list_of_qas:\n",
    "            rewritten = quest['new_question']\n",
    "            answer = ann['answers'][0]['answer']\n",
    "            answer_id = ann['answers'][0]['mturk_id']\n",
    "            cluster_dict = {\"answer\": answer, \"id\": answer_id} \n",
    "            clusters[rewritten].append(cluster_dict)\n",
    "        clusters_by_qid[qid] = clusters\n",
    "    return clusters_by_qid\n",
    "\n",
    "# get the clusters from kmeans preprocessing\n",
    "def get_preprocessed_clusters(questions, annotations): \n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters_by_qid = {}\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        clusters = defaultdict(list)\n",
    "        for quest, ann in list_of_qas:\n",
    "            answer = ann['answers'][0]['answer']\n",
    "            answer_id = ann['answers'][0]['mturk_id']\n",
    "            id_key, answer_id_suffix = answer_id.split(\".\")\n",
    "            cluster_dict = {\"answer\": answer, \"id\": answer_id} \n",
    "            clusters[id_key].append(cluster_dict)\n",
    "        clusters_by_qid[qid] = clusters\n",
    "    return clusters_by_qid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "curr_path = pathlib.Path('').resolve().parent\n",
    "sys.path.insert(0, str(curr_path.joinpath(\"hit3.0\").joinpath(\"results\")))\n",
    "from process_csv import f1_score\n",
    "\n",
    "def preprocess(cluster_data):\n",
    "    if type(cluster_data) in [dict, defaultdict]:\n",
    "        # dealing with predicted clusters or preprocessed clusters\n",
    "        return cluster_data.values()\n",
    "    return cluster_data\n",
    "\n",
    "def get_scores(clusters_by_qid_a, clusters_by_qid_b):\n",
    "    scores = []\n",
    "    for qid in clusters_by_qid_a.keys():\n",
    "        cluster_a = preprocess(clusters_by_qid_a[qid])\n",
    "        cluster_b = preprocess(clusters_by_qid_b[qid])\n",
    "        f1_tuple = f1_score(cluster_a, cluster_b)\n",
    "        f1_tuple = f1_tuple[0:-1]\n",
    "        scores.append(f1_tuple)\n",
    "    # print(scores)\n",
    "    scores = np.array(scores)\n",
    "    return np.mean(scores, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string_metrics import BertSimilarityScore\n",
    "score_cls = BertSimilarityScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "import scipy \n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def read_generations(output_path):\n",
    "    flat_data_by_qid = {}\n",
    "    data = open(output_path).readlines()\n",
    "    for line in data:\n",
    "        batch_data = json.loads(line)\n",
    "        for qid, generation in zip(batch_data['question_id'], batch_data['speaker_utterances'][0]):\n",
    "            flat_data_by_qid[qid] = generation\n",
    "    return flat_data_by_qid\n",
    "\n",
    "def clean_text(text): \n",
    "    text = re.sub(\"<.*?>\", \"\", text)\n",
    "    text = text.strip() \n",
    "    return text \n",
    "\n",
    "# get the clusters from predictions \n",
    "def get_prediction_clusters(predictions_jsonl,\n",
    "                            questions, \n",
    "                            annotations, \n",
    "                            score_cls, \n",
    "                            t = 1.06, \n",
    "                            criterion = \"centroid\", \n",
    "                            method = \"distance\"):\n",
    "    generations_by_qid = read_generations(predictions_jsonl)\n",
    "    anns_by_qid = defaultdict(list)\n",
    "    answers_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        generation = clean_text(generations_by_qid[quest['question_id']])\n",
    "        anns_by_qid[qid].append(generation)\n",
    "        answers_by_qid[qid].append(ann['answers'])\n",
    "\n",
    "    scores_by_qid = {} \n",
    "    clusts_by_qid = {}\n",
    "    # Get matrix of scores \n",
    "    answer_clusters = {}\n",
    "    for qid, quest_list in tqdm(anns_by_qid.items()): \n",
    "        scores = np.zeros((len(quest_list), len(quest_list))) \n",
    "        done = []\n",
    "        for i, q1 in enumerate(quest_list): \n",
    "            for j, q2 in enumerate(quest_list):\n",
    "                if i == j: \n",
    "                    scores[i,j] = 0.0 \n",
    "                    continue\n",
    "                sim_score = score_cls.get_similarity(q1, q2) \n",
    "                scores[i,j] = 1/sim_score\n",
    "                done.append((i,j))\n",
    "                done.append((j,i))\n",
    "\n",
    "        scores_by_qid[qid] = scores \n",
    "        scores = scipy.spatial.distance.squareform(scores)\n",
    "        link = linkage(scores, method=method, metric=\"cosine\")\n",
    "        clust = fcluster(link, t=t, criterion=criterion)\n",
    "\n",
    "        clusts_by_qid[qid] = clust \n",
    "        answers_clustered = defaultdict(list)\n",
    "        ans_list = answers_by_qid[qid]\n",
    "        for i, idx in enumerate(clust):\n",
    "            answer = ans_list[i]\n",
    "            orig_id = answer[0]['mturk_id']\n",
    "            cluster_dict = {\"answer\": answer[0]['answer'], \"question\": quest_list[i], \"id\": orig_id} \n",
    "            answers_clustered[f\"g{idx}\"].append(cluster_dict)\n",
    "        answer_clusters[qid] = answers_clustered\n",
    "\n",
    "    return answer_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:26<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P: 65.91, R: 100.00, F1: 76.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import logging\n",
    "logging.set_verbosity(50)\n",
    "\n",
    "pred_path = \"/brtx/602-nvme1/estengel/annotator_uncertainty/models/img2q_t5_base_no_limit/output/dev_set_predictions.jsonl\"\n",
    "pred_clusters = get_prediction_clusters(pred_path,\n",
    "                                        questions, \n",
    "                                        annotations,\n",
    "                                        score_cls=score_cls,\n",
    "                                        criterion=\"distance\",\n",
    "                                        method=\"centroid\",\n",
    "                                        t=1.06)\n",
    "\n",
    "ann_clusters = get_annotator_clusters(questions, annotations)\n",
    "\n",
    "pred_to_ann = get_scores(pred_clusters, ann_clusters)\n",
    "print(f\"P: {pred_to_ann[1]*100:.2f}, R: {pred_to_ann[2]*100:.2f}, F1: {pred_to_ann[0]*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:26<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.0, P: 90.91, R: 96.21, F1: 91.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:27<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.01, P: 87.88, R: 98.48, F1: 91.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:26<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.02, P: 81.06, R: 98.48, F1: 85.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:26<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.03, P: 81.06, R: 98.48, F1: 85.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:26<00:00,  2.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.04, P: 81.06, R: 98.48, F1: 85.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:25<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.05, P: 76.52, R: 100.00, F1: 83.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:25<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.06, P: 65.91, R: 100.00, F1: 76.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:25<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.07, P: 65.91, R: 100.00, F1: 76.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:25<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 0.08, P: 100.00, R: 94.70, F1: 96.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:25<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method: centroid, Crit: distance, t: 1.09, P: 59.09, R: 100.00, F1: 70.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:08<00:09,  1.66s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb Cell 8\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=9'>10</a>\u001b[0m     t_choices \u001b[39m=\u001b[39m [\u001b[39m0.0\u001b[39m, \u001b[39m1.0\u001b[39m, \u001b[39m2.0\u001b[39m, \u001b[39m3.0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m t_choices:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=11'>12</a>\u001b[0m     pred_clusters \u001b[39m=\u001b[39m get_prediction_clusters(pred_path,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=12'>13</a>\u001b[0m                                     questions, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=13'>14</a>\u001b[0m                                     annotations,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=14'>15</a>\u001b[0m                                     score_cls\u001b[39m=\u001b[39;49mscore_cls,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=15'>16</a>\u001b[0m                                     criterion\u001b[39m=\u001b[39;49mcrit,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=16'>17</a>\u001b[0m                                     method \u001b[39m=\u001b[39;49m method,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=17'>18</a>\u001b[0m                                     t\u001b[39m=\u001b[39;49mt)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=19'>20</a>\u001b[0m     pred_to_ann \u001b[39m=\u001b[39m get_scores(pred_clusters, ann_clusters)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMethod: \u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m}\u001b[39;00m\u001b[39m, Crit: \u001b[39m\u001b[39m{\u001b[39;00mcrit\u001b[39m}\u001b[39;00m\u001b[39m, t: \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m}\u001b[39;00m\u001b[39m, P: \u001b[39m\u001b[39m{\u001b[39;00mpred_to_ann[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, R: \u001b[39m\u001b[39m{\u001b[39;00mpred_to_ann[\u001b[39m2\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, F1: \u001b[39m\u001b[39m{\u001b[39;00mpred_to_ann[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb Cell 8\u001b[0m in \u001b[0;36mget_prediction_clusters\u001b[0;34m(predictions_jsonl, questions, annotations, score_cls, t, criterion, method)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=46'>47</a>\u001b[0m     scores[i,j] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=47'>48</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=48'>49</a>\u001b[0m sim_score \u001b[39m=\u001b[39m score_cls\u001b[39m.\u001b[39;49mget_similarity(q1, q2) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=49'>50</a>\u001b[0m scores[i,j] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39msim_score\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bblab/home/estengel/annotator_uncertainty/analysis/text_clusters.ipynb#ch0000008vscode-remote?line=50'>51</a>\u001b[0m done\u001b[39m.\u001b[39mappend((i,j))\n",
      "File \u001b[0;32m~/annotator_uncertainty/analysis/string_metrics.py:44\u001b[0m, in \u001b[0;36mBertSimilarityScore.get_similarity\u001b[0;34m(self, sentence_1, sentence_2)\u001b[0m\n\u001b[1;32m     42\u001b[0m format_sent_1 \u001b[39m=\u001b[39m [sentence_1]\n\u001b[1;32m     43\u001b[0m format_sent_2 \u001b[39m=\u001b[39m [sentence_2]\n\u001b[0;32m---> 44\u001b[0m p, r, f1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscorer\u001b[39m.\u001b[39;49mscore(format_sent_1, format_sent_2, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     45\u001b[0m \u001b[39m# print(f\"BERT Score: P={P.mean().item():.6f} R={R.mean().item():.6f} F={F1.mean().item():.6f}\")\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m f1\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/bert_score/scorer.py:213\u001b[0m, in \u001b[0;36mBERTScorer.score\u001b[0;34m(self, cands, refs, verbose, batch_size, return_hash)\u001b[0m\n\u001b[1;32m    210\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39msep_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    211\u001b[0m     idf_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer\u001b[39m.\u001b[39mcls_token_id] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 213\u001b[0m all_preds \u001b[39m=\u001b[39m bert_cos_score_idf(\n\u001b[1;32m    214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_model,\n\u001b[1;32m    215\u001b[0m     refs,\n\u001b[1;32m    216\u001b[0m     cands,\n\u001b[1;32m    217\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer,\n\u001b[1;32m    218\u001b[0m     idf_dict,\n\u001b[1;32m    219\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    220\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    221\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    222\u001b[0m     all_layers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_layers,\n\u001b[1;32m    223\u001b[0m )\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    225\u001b[0m \u001b[39mif\u001b[39;00m ref_group_boundaries \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     max_preds \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/bert_score/utils.py:528\u001b[0m, in \u001b[0;36mbert_cos_score_idf\u001b[0;34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[39mfor\u001b[39;00m batch_start \u001b[39min\u001b[39;00m iter_range:\n\u001b[1;32m    527\u001b[0m     sen_batch \u001b[39m=\u001b[39m sentences[batch_start : batch_start \u001b[39m+\u001b[39m batch_size]\n\u001b[0;32m--> 528\u001b[0m     embs, masks, padded_idf \u001b[39m=\u001b[39m get_bert_embedding(\n\u001b[1;32m    529\u001b[0m         sen_batch, model, tokenizer, idf_dict, device\u001b[39m=\u001b[39;49mdevice, all_layers\u001b[39m=\u001b[39;49mall_layers\n\u001b[1;32m    530\u001b[0m     )\n\u001b[1;32m    531\u001b[0m     embs \u001b[39m=\u001b[39m embs\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m    532\u001b[0m     masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/bert_score/utils.py:407\u001b[0m, in \u001b[0;36mget_bert_embedding\u001b[0;34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    406\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(all_sens), batch_size):\n\u001b[0;32m--> 407\u001b[0m         batch_embedding \u001b[39m=\u001b[39m bert_encode(\n\u001b[1;32m    408\u001b[0m             model, padded_sens[i : i \u001b[39m+\u001b[39;49m batch_size], attention_mask\u001b[39m=\u001b[39;49mmask[i : i \u001b[39m+\u001b[39;49m batch_size], all_layers\u001b[39m=\u001b[39;49mall_layers,\n\u001b[1;32m    409\u001b[0m         )\n\u001b[1;32m    410\u001b[0m         embeddings\u001b[39m.\u001b[39mappend(batch_embedding)\n\u001b[1;32m    411\u001b[0m         \u001b[39mdel\u001b[39;00m batch_embedding\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/bert_score/utils.py:318\u001b[0m, in \u001b[0;36mbert_encode\u001b[0;34m(model, x, attention_mask, all_layers)\u001b[0m\n\u001b[1;32m    316\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m    317\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 318\u001b[0m     out \u001b[39m=\u001b[39m model(x, attention_mask\u001b[39m=\u001b[39;49mattention_mask, output_hidden_states\u001b[39m=\u001b[39;49mall_layers)\n\u001b[1;32m    319\u001b[0m \u001b[39mif\u001b[39;00m all_layers:\n\u001b[1;32m    320\u001b[0m     emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:848\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    839\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    841\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    842\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    843\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    846\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    847\u001b[0m )\n\u001b[0;32m--> 848\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    849\u001b[0m     embedding_output,\n\u001b[1;32m    850\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    851\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    852\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    853\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    854\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    855\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    856\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    857\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    858\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    859\u001b[0m )\n\u001b[1;32m    860\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    861\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:524\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    515\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    516\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    517\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    525\u001b[0m         hidden_states,\n\u001b[1;32m    526\u001b[0m         attention_mask,\n\u001b[1;32m    527\u001b[0m         layer_head_mask,\n\u001b[1;32m    528\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    529\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    530\u001b[0m         past_key_value,\n\u001b[1;32m    531\u001b[0m         output_attentions,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    534\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:451\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    448\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    449\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 451\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    452\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    453\u001b[0m )\n\u001b[1;32m    454\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    456\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:464\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[1;32m    463\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 464\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[1;32m    465\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:375\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 375\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    376\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    377\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/brtx/601-nvme1/estengel/miniconda3/envs/cert/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# BEST: Method: centroid, Crit: distance, t: 1.06, P: 69.21, R: 96.66, F1: 77.83\n",
    "# Not really, it either predicts a single cluster for everything or puts everything in its own cluster, which sucks \n",
    "for method in [\"centroid\"]:\n",
    "    # for crit in ['inconsistent', 'distance']: \n",
    "    for crit in ['distance']:\n",
    "        if crit == \"distance\":\n",
    "            t_choices = [1.00, 1.01, 1.02, 1.03,  1.04,  1.05,  1.06, 1.07, 0.08, 1.09, 1.10] \n",
    "            # t_choices = [1.00,   1.05,   1.10] \n",
    "        else:\n",
    "            t_choices = [0.0, 1.0, 2.0, 3.0]\n",
    "        for t in t_choices:\n",
    "            pred_clusters = get_prediction_clusters(pred_path,\n",
    "                                            questions, \n",
    "                                            annotations,\n",
    "                                            score_cls=score_cls,\n",
    "                                            criterion=crit,\n",
    "                                            method = method,\n",
    "                                            t=t)\n",
    "\n",
    "            pred_to_ann = get_scores(pred_clusters, ann_clusters)\n",
    "            print(f\"Method: {method}, Crit: {crit}, t: {t}, P: {pred_to_ann[1]*100:.2f}, R: {pred_to_ann[2]*100:.2f}, F1: {pred_to_ann[0]*100:.2f}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a71203a0affc7207a6dfd2214cf786ae35ec96fb0be57392a936686760a696bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
