{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "from pathlib import Path \n",
    "import os \n",
    "import subprocess\n",
    "data_path = \"/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/dev_from_mturk_small/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def min_gen(checkpoint, steps=200, lr=0.05, device=7, beta_text_loss=0.0):\n",
    "    path = os.environ.get(\"PATH\")\n",
    "    pythonpath = os.environ.get(\"PYTHONPATH\")\n",
    "    env_vars = {\"ALLENNLP_CACHE_ROOT\": \"/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/\",\n",
    "                \"CHECKPOINT_DIR\": checkpoint,\n",
    "                \"TEST_DATA\": data_path,\n",
    "                \"CUDA_VISIBLE_DEVICES\": str(device),\n",
    "                \"PYTHONPATH\": f\"/home/estengel/annotator_uncertainty/models:{pythonpath}\",\n",
    "                \"PATH\": f\"/home/estengel/annotator_uncertainty/models/:{path}\"}\n",
    "    os.environ.update(env_vars)\n",
    "    p = subprocess.Popen([\"mkdir\", \"-p\", \"${CHECKPOINT_DIR}/output\"], stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "\n",
    "    command_str = f\"\"\"python -um allennlp min_gen \\\n",
    "    --include-package allennlp.data.dataset_readers \\\n",
    "    --include-package allennlp.training \\\n",
    "    {checkpoint}/ckpt/model.tar.gz \\\n",
    "    {data_path} \\\n",
    "    --cuda-device 0 \\\n",
    "    --predictions-output-file {checkpoint}/output/dev_min_gen_debug_steps_{steps}_lr_{lr}.jsonl \\\n",
    "    --descent-strategy steps \\\n",
    "    --num-descent-steps {steps} \\\n",
    "    --lr {lr}\n",
    "    \"\"\"\n",
    "    # --beta-text-loss {beta_text_loss} \\\n",
    "\n",
    "    p = subprocess.Popen(command_str, shell=True, stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    print(out.decode(\"utf8\")) \n",
    "\n",
    "    with open(f\"{checkpoint}/output/dev_min_gen_debug_steps_{steps}_lr_{lr}.jsonl\") as f1:\n",
    "        output_data = [json.loads(l) for l in f1]\n",
    "    return out, err, output_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqa_loss: 5.641478061676025\n",
      "vqa_loss: 1.9452993869781494\n",
      "vqa_loss: 1.1624641418457031\n",
      "vqa_loss: 0.9108308553695679\n",
      "vqa_loss: 0.8077822923660278\n",
      "vqa_loss: 0.7641540169715881\n",
      "vqa_loss: 0.7449965476989746\n",
      "vqa_loss: 0.7357717156410217\n",
      "vqa_loss: 0.7305828928947449\n",
      "vqa_loss: 0.7270805239677429\n",
      "vqa_loss: 0.7243292331695557\n",
      "vqa_loss: 0.7219557762145996\n",
      "vqa_loss: 0.7197996973991394\n",
      "vqa_loss: 0.7178025245666504\n",
      "vqa_loss: 0.7159245610237122\n",
      "vqa_loss: 0.7141520977020264\n",
      "vqa_loss: 0.712470531463623\n",
      "vqa_loss: 0.7108775973320007\n",
      "vqa_loss: 0.7093620300292969\n",
      "vqa_loss: 0.7079180479049683\n",
      "vqa_loss: 0.7065392732620239\n",
      "vqa_loss: 0.7052227854728699\n",
      "vqa_loss: 0.7039636969566345\n",
      "vqa_loss: 0.7027553915977478\n",
      "vqa_loss: 0.7015970945358276\n",
      "vqa_loss: 0.7004887461662292\n",
      "vqa_loss: 0.6994245052337646\n",
      "vqa_loss: 0.6984009146690369\n",
      "vqa_loss: 0.6974160671234131\n",
      "vqa_loss: 0.6964667439460754\n",
      "vqa_loss: 0.6955491900444031\n",
      "vqa_loss: 0.694667637348175\n",
      "vqa_loss: 0.6938158273696899\n",
      "vqa_loss: 0.692990243434906\n",
      "vqa_loss: 0.6921957731246948\n",
      "vqa_loss: 0.6914254426956177\n",
      "vqa_loss: 0.6906785368919373\n",
      "vqa_loss: 0.6899580359458923\n",
      "vqa_loss: 0.6892582774162292\n",
      "vqa_loss: 0.6885768175125122\n",
      "vqa_loss: 0.6879168152809143\n",
      "vqa_loss: 0.6872769594192505\n",
      "vqa_loss: 0.6866552829742432\n",
      "vqa_loss: 0.6860511898994446\n",
      "vqa_loss: 0.6854628920555115\n",
      "vqa_loss: 0.6848929524421692\n",
      "vqa_loss: 0.6843376159667969\n",
      "vqa_loss: 0.6837952733039856\n",
      "vqa_loss: 0.683268666267395\n",
      "vqa_loss: 0.6827535629272461\n",
      "vqa_loss: 0.6822530031204224\n",
      "vqa_loss: 0.681766152381897\n",
      "vqa_loss: 0.6812872290611267\n",
      "vqa_loss: 0.6808199882507324\n",
      "vqa_loss: 0.6803684234619141\n",
      "vqa_loss: 0.6799246668815613\n",
      "vqa_loss: 0.679491400718689\n",
      "vqa_loss: 0.6790671944618225\n",
      "vqa_loss: 0.6786531209945679\n",
      "vqa_loss: 0.6782500147819519\n",
      "vqa_loss: 0.6778541207313538\n",
      "vqa_loss: 0.6774653792381287\n",
      "vqa_loss: 0.6770845651626587\n",
      "vqa_loss: 0.6767117977142334\n",
      "vqa_loss: 0.6763489246368408\n",
      "vqa_loss: 0.6759913563728333\n",
      "vqa_loss: 0.6756434440612793\n",
      "vqa_loss: 0.6753029227256775\n",
      "vqa_loss: 0.6749686002731323\n",
      "vqa_loss: 0.6746391654014587\n",
      "vqa_loss: 0.6743168234825134\n",
      "vqa_loss: 0.6739988923072815\n",
      "vqa_loss: 0.6736881136894226\n",
      "vqa_loss: 0.6733819246292114\n",
      "vqa_loss: 0.6730830669403076\n",
      "vqa_loss: 0.672791600227356\n",
      "vqa_loss: 0.6725037693977356\n",
      "vqa_loss: 0.6722202301025391\n",
      "vqa_loss: 0.6719428896903992\n",
      "vqa_loss: 0.6716701984405518\n",
      "vqa_loss: 0.6714009046554565\n",
      "vqa_loss: 0.6711370944976807\n",
      "vqa_loss: 0.6708759069442749\n",
      "vqa_loss: 0.670620322227478\n",
      "vqa_loss: 0.6703724265098572\n",
      "vqa_loss: 0.6701257228851318\n",
      "vqa_loss: 0.6698857545852661\n",
      "vqa_loss: 0.6696462631225586\n",
      "vqa_loss: 0.6694129705429077\n",
      "vqa_loss: 0.6691809296607971\n",
      "vqa_loss: 0.6689537167549133\n",
      "vqa_loss: 0.6687295436859131\n",
      "vqa_loss: 0.6685082316398621\n",
      "vqa_loss: 0.6682925224304199\n",
      "vqa_loss: 0.6680788993835449\n",
      "vqa_loss: 0.6678693890571594\n",
      "vqa_loss: 0.6676602959632874\n",
      "vqa_loss: 0.667454719543457\n",
      "vqa_loss: 0.6672529578208923\n",
      "vqa_loss: 0.6670525670051575\n",
      "vqa_loss: 0.6668585538864136\n",
      "vqa_loss: 0.6666674017906189\n",
      "vqa_loss: 0.6664780378341675\n",
      "vqa_loss: 0.6662914156913757\n",
      "vqa_loss: 0.6661062240600586\n",
      "vqa_loss: 0.6659239530563354\n",
      "vqa_loss: 0.6657459735870361\n",
      "vqa_loss: 0.6655685305595398\n",
      "vqa_loss: 0.6653934717178345\n",
      "vqa_loss: 0.6652206778526306\n",
      "vqa_loss: 0.6650509238243103\n",
      "vqa_loss: 0.6648834943771362\n",
      "vqa_loss: 0.6647188067436218\n",
      "vqa_loss: 0.664553701877594\n",
      "vqa_loss: 0.664393424987793\n",
      "vqa_loss: 0.6642329692840576\n",
      "vqa_loss: 0.6640766859054565\n",
      "vqa_loss: 0.6639212369918823\n",
      "vqa_loss: 0.6637675762176514\n",
      "vqa_loss: 0.6636158227920532\n",
      "vqa_loss: 0.663466215133667\n",
      "vqa_loss: 0.6633182168006897\n",
      "vqa_loss: 0.6631741523742676\n",
      "vqa_loss: 0.6630281209945679\n",
      "vqa_loss: 0.6628845930099487\n",
      "vqa_loss: 0.6627444624900818\n",
      "vqa_loss: 0.6626061201095581\n",
      "vqa_loss: 0.662468433380127\n",
      "vqa_loss: 0.6623318195343018\n",
      "vqa_loss: 0.6621987819671631\n",
      "vqa_loss: 0.6620646119117737\n",
      "vqa_loss: 0.661932647228241\n",
      "vqa_loss: 0.6618028879165649\n",
      "vqa_loss: 0.6616748571395874\n",
      "vqa_loss: 0.6615490317344666\n",
      "vqa_loss: 0.6614216566085815\n",
      "vqa_loss: 0.661298394203186\n",
      "vqa_loss: 0.661175012588501\n",
      "vqa_loss: 0.6610537171363831\n",
      "vqa_loss: 0.660934329032898\n",
      "vqa_loss: 0.6608152985572815\n",
      "vqa_loss: 0.6606974005699158\n",
      "vqa_loss: 0.6605813503265381\n",
      "vqa_loss: 0.6604664921760559\n",
      "vqa_loss: 0.6603530645370483\n",
      "vqa_loss: 0.6602389216423035\n",
      "vqa_loss: 0.6601287126541138\n",
      "vqa_loss: 0.6600173115730286\n",
      "vqa_loss: 0.6599072813987732\n",
      "vqa_loss: 0.65980064868927\n",
      "vqa_loss: 0.6596924066543579\n",
      "vqa_loss: 0.6595861315727234\n",
      "vqa_loss: 0.6594812870025635\n",
      "vqa_loss: 0.6593780517578125\n",
      "vqa_loss: 0.6592745184898376\n",
      "vqa_loss: 0.6591731309890747\n",
      "vqa_loss: 0.6590739488601685\n",
      "vqa_loss: 0.6589735746383667\n",
      "vqa_loss: 0.6588761806488037\n",
      "vqa_loss: 0.658778190612793\n",
      "vqa_loss: 0.6586818695068359\n",
      "vqa_loss: 0.6585848331451416\n",
      "vqa_loss: 0.6584900617599487\n",
      "vqa_loss: 0.6583978533744812\n",
      "vqa_loss: 0.6583043336868286\n",
      "vqa_loss: 0.6582100987434387\n",
      "vqa_loss: 0.658119261264801\n",
      "vqa_loss: 0.658028781414032\n",
      "vqa_loss: 0.6579393744468689\n",
      "vqa_loss: 0.6578514575958252\n",
      "vqa_loss: 0.6577624082565308\n",
      "vqa_loss: 0.6576759815216064\n",
      "vqa_loss: 0.6575896739959717\n",
      "vqa_loss: 0.6575015783309937\n",
      "vqa_loss: 0.6574162840843201\n",
      "vqa_loss: 0.6573324203491211\n",
      "vqa_loss: 0.6572492718696594\n",
      "vqa_loss: 0.6571663618087769\n",
      "vqa_loss: 0.657084584236145\n",
      "vqa_loss: 0.6570023894309998\n",
      "vqa_loss: 0.6569222807884216\n",
      "vqa_loss: 0.656842052936554\n",
      "vqa_loss: 0.656762421131134\n",
      "vqa_loss: 0.6566839218139648\n",
      "vqa_loss: 0.6566067337989807\n",
      "vqa_loss: 0.6565303206443787\n",
      "vqa_loss: 0.6564539670944214\n",
      "vqa_loss: 0.6563778519630432\n",
      "vqa_loss: 0.6563028693199158\n",
      "vqa_loss: 0.6562277674674988\n",
      "vqa_loss: 0.6561541557312012\n",
      "vqa_loss: 0.6560806035995483\n",
      "vqa_loss: 0.6560083031654358\n",
      "vqa_loss: 0.655935525894165\n",
      "vqa_loss: 0.6558647155761719\n",
      "vqa_loss: 0.6557943820953369\n",
      "vqa_loss: 0.6557227373123169\n",
      "vqa_loss: 0.6556519269943237\n",
      "vqa_loss: 0.655582845211029\n",
      "vqa_loss: 0.6555130481719971\n",
      "vqa_loss: 0.047036971896886826\n",
      "vqa_loss: 0.04407690837979317\n",
      "vqa_loss: 0.041559625416994095\n",
      "vqa_loss: 0.03939325734972954\n",
      "vqa_loss: 0.03751368820667267\n",
      "vqa_loss: 0.035865623503923416\n",
      "vqa_loss: 0.034409694373607635\n",
      "vqa_loss: 0.033113524317741394\n",
      "vqa_loss: 0.0319531224668026\n",
      "vqa_loss: 0.03090818040072918\n",
      "vqa_loss: 0.02996089495718479\n",
      "vqa_loss: 0.029101042076945305\n",
      "vqa_loss: 0.028314154595136642\n",
      "vqa_loss: 0.027595264837145805\n",
      "vqa_loss: 0.026931149885058403\n",
      "vqa_loss: 0.02631678804755211\n",
      "vqa_loss: 0.02574653923511505\n",
      "vqa_loss: 0.025217732414603233\n",
      "vqa_loss: 0.024724816903471947\n",
      "vqa_loss: 0.02426021732389927\n",
      "vqa_loss: 0.023826349526643753\n",
      "vqa_loss: 0.02342003583908081\n",
      "vqa_loss: 0.023035574704408646\n",
      "vqa_loss: 0.022673947736620903\n",
      "vqa_loss: 0.02233278937637806\n",
      "vqa_loss: 0.022008663043379784\n",
      "vqa_loss: 0.021698128432035446\n",
      "vqa_loss: 0.021404890343546867\n",
      "vqa_loss: 0.02112716995179653\n",
      "vqa_loss: 0.020861642435193062\n",
      "vqa_loss: 0.020607365295290947\n",
      "vqa_loss: 0.02036338858306408\n",
      "vqa_loss: 0.020129121840000153\n",
      "vqa_loss: 0.019906003028154373\n",
      "vqa_loss: 0.01969153992831707\n",
      "vqa_loss: 0.01948465406894684\n",
      "vqa_loss: 0.019287029281258583\n",
      "vqa_loss: 0.019094128161668777\n",
      "vqa_loss: 0.018909655511379242\n",
      "vqa_loss: 0.018731113523244858\n",
      "vqa_loss: 0.018558261916041374\n",
      "vqa_loss: 0.018390744924545288\n",
      "vqa_loss: 0.018229521811008453\n",
      "vqa_loss: 0.018073998391628265\n",
      "vqa_loss: 0.017922621220350266\n",
      "vqa_loss: 0.01777671091258526\n",
      "vqa_loss: 0.017634596675634384\n",
      "vqa_loss: 0.017495563253760338\n",
      "vqa_loss: 0.01736009120941162\n",
      "vqa_loss: 0.017229845747351646\n",
      "vqa_loss: 0.017103999853134155\n",
      "vqa_loss: 0.016980526968836784\n",
      "vqa_loss: 0.01686001569032669\n",
      "vqa_loss: 0.016741763800382614\n",
      "vqa_loss: 0.016628146171569824\n",
      "vqa_loss: 0.016514401882886887\n",
      "vqa_loss: 0.01640469953417778\n",
      "vqa_loss: 0.016297733411192894\n",
      "vqa_loss: 0.01619468815624714\n",
      "vqa_loss: 0.016093427315354347\n",
      "vqa_loss: 0.015989413484930992\n",
      "vqa_loss: 0.015892665833234787\n",
      "vqa_loss: 0.01579698547720909\n",
      "vqa_loss: 0.01570320315659046\n",
      "vqa_loss: 0.015613829717040062\n",
      "vqa_loss: 0.01552373357117176\n",
      "vqa_loss: 0.015434346161782742\n",
      "vqa_loss: 0.015347935259342194\n",
      "vqa_loss: 0.015262235887348652\n",
      "vqa_loss: 0.015177363529801369\n",
      "vqa_loss: 0.01509594451636076\n",
      "vqa_loss: 0.01501500140875578\n",
      "vqa_loss: 0.014936675317585468\n",
      "vqa_loss: 0.014858701266348362\n",
      "vqa_loss: 0.014783228747546673\n",
      "vqa_loss: 0.014707157388329506\n",
      "vqa_loss: 0.014632035046815872\n",
      "vqa_loss: 0.014558817259967327\n",
      "vqa_loss: 0.014488576911389828\n",
      "vqa_loss: 0.014417142607271671\n",
      "vqa_loss: 0.014348565600812435\n",
      "vqa_loss: 0.014280700124800205\n",
      "vqa_loss: 0.014213192276656628\n",
      "vqa_loss: 0.014147707261145115\n",
      "vqa_loss: 0.014080788940191269\n",
      "vqa_loss: 0.014016850851476192\n",
      "vqa_loss: 0.013952788896858692\n",
      "vqa_loss: 0.013890395872294903\n",
      "vqa_loss: 0.013826927170157433\n",
      "vqa_loss: 0.013766317628324032\n",
      "vqa_loss: 0.013705111108720303\n",
      "vqa_loss: 0.01364842988550663\n",
      "vqa_loss: 0.013589009642601013\n",
      "vqa_loss: 0.01352827437222004\n",
      "vqa_loss: 0.013473856262862682\n",
      "vqa_loss: 0.013418004848062992\n",
      "vqa_loss: 0.013362272642552853\n",
      "vqa_loss: 0.013306180015206337\n",
      "vqa_loss: 0.013251874595880508\n",
      "vqa_loss: 0.013196497224271297\n",
      "vqa_loss: 0.013143499381840229\n",
      "vqa_loss: 0.01308966800570488\n",
      "vqa_loss: 0.013039292767643929\n",
      "vqa_loss: 0.012987366877496243\n",
      "vqa_loss: 0.01293567568063736\n",
      "vqa_loss: 0.012886728160083294\n",
      "vqa_loss: 0.012835752218961716\n",
      "vqa_loss: 0.0127864433452487\n",
      "vqa_loss: 0.012737372890114784\n",
      "vqa_loss: 0.012690804898738861\n",
      "vqa_loss: 0.01264363992959261\n",
      "vqa_loss: 0.012597426772117615\n",
      "vqa_loss: 0.012550976127386093\n",
      "vqa_loss: 0.01250512059777975\n",
      "vqa_loss: 0.012459023855626583\n",
      "vqa_loss: 0.01241304725408554\n",
      "vqa_loss: 0.012368500232696533\n",
      "vqa_loss: 0.012324071489274502\n",
      "vqa_loss: 0.01228119246661663\n",
      "vqa_loss: 0.012238669209182262\n",
      "vqa_loss: 0.01219459529966116\n",
      "vqa_loss: 0.012151595205068588\n",
      "vqa_loss: 0.012106805108487606\n",
      "vqa_loss: 0.012066545896232128\n",
      "vqa_loss: 0.012023662216961384\n",
      "vqa_loss: 0.011984474025666714\n",
      "vqa_loss: 0.011945164762437344\n",
      "vqa_loss: 0.011904546059668064\n",
      "vqa_loss: 0.011864879168570042\n",
      "vqa_loss: 0.011824018321931362\n",
      "vqa_loss: 0.011784827336668968\n",
      "vqa_loss: 0.01174647081643343\n",
      "vqa_loss: 0.011707755737006664\n",
      "vqa_loss: 0.011670112609863281\n",
      "vqa_loss: 0.011632350273430347\n",
      "vqa_loss: 0.011595780029892921\n",
      "vqa_loss: 0.011557777412235737\n",
      "vqa_loss: 0.011523351073265076\n",
      "vqa_loss: 0.011487256735563278\n",
      "vqa_loss: 0.011449969373643398\n",
      "vqa_loss: 0.011413873173296452\n",
      "vqa_loss: 0.01137861143797636\n",
      "vqa_loss: 0.011342037469148636\n",
      "vqa_loss: 0.01130772940814495\n",
      "vqa_loss: 0.01127318199723959\n",
      "vqa_loss: 0.01123660709708929\n",
      "vqa_loss: 0.011203370057046413\n",
      "vqa_loss: 0.011168940924108028\n",
      "vqa_loss: 0.011134987697005272\n",
      "vqa_loss: 0.011102106422185898\n",
      "vqa_loss: 0.011068272404372692\n",
      "vqa_loss: 0.01103503443300724\n",
      "vqa_loss: 0.011002630926668644\n",
      "vqa_loss: 0.010970464907586575\n",
      "vqa_loss: 0.0109375836327672\n",
      "vqa_loss: 0.010905653238296509\n",
      "vqa_loss: 0.010874559171497822\n",
      "vqa_loss: 0.010842153802514076\n",
      "vqa_loss: 0.010811416432261467\n",
      "vqa_loss: 0.010780202224850655\n",
      "vqa_loss: 0.01074874959886074\n",
      "vqa_loss: 0.010717416182160378\n",
      "vqa_loss: 0.01068679615855217\n",
      "vqa_loss: 0.010654270648956299\n",
      "vqa_loss: 0.010625796392560005\n",
      "vqa_loss: 0.01059648860245943\n",
      "vqa_loss: 0.010565750300884247\n",
      "vqa_loss: 0.010537631809711456\n",
      "vqa_loss: 0.010509038344025612\n",
      "vqa_loss: 0.010478896088898182\n",
      "vqa_loss: 0.010450540110468864\n",
      "vqa_loss: 0.01042194664478302\n",
      "vqa_loss: 0.010394305922091007\n",
      "vqa_loss: 0.010364041663706303\n",
      "vqa_loss: 0.010336518287658691\n",
      "vqa_loss: 0.010307567194104195\n",
      "vqa_loss: 0.010279209353029728\n",
      "vqa_loss: 0.01025144848972559\n",
      "vqa_loss: 0.010223687626421452\n",
      "vqa_loss: 0.010199383832514286\n",
      "vqa_loss: 0.010171146132051945\n",
      "vqa_loss: 0.010144099593162537\n",
      "vqa_loss: 0.01011729147285223\n",
      "vqa_loss: 0.010090363211929798\n",
      "vqa_loss: 0.010063556022942066\n",
      "vqa_loss: 0.01003746222704649\n",
      "vqa_loss: 0.010011846199631691\n",
      "vqa_loss: 0.00998587068170309\n",
      "vqa_loss: 0.009960968978703022\n",
      "vqa_loss: 0.00993475690484047\n",
      "vqa_loss: 0.00991045031696558\n",
      "vqa_loss: 0.00988459400832653\n",
      "vqa_loss: 0.00985873956233263\n",
      "vqa_loss: 0.009833957068622112\n",
      "vqa_loss: 0.009811198338866234\n",
      "vqa_loss: 0.00978474598377943\n",
      "vqa_loss: 0.009761751629412174\n",
      "vqa_loss: 0.009737682528793812\n",
      "vqa_loss: 0.009714210405945778\n",
      "vqa_loss: 0.009689426980912685\n",
      "vqa_loss: 0.13642440736293793\n",
      "vqa_loss: 0.10912740230560303\n",
      "vqa_loss: 0.09144183248281479\n",
      "vqa_loss: 0.07912469655275345\n",
      "vqa_loss: 0.07008878141641617\n",
      "vqa_loss: 0.0631931871175766\n",
      "vqa_loss: 0.05776388198137283\n",
      "vqa_loss: 0.05338170751929283\n",
      "vqa_loss: 0.04977043345570564\n",
      "vqa_loss: 0.04674581065773964\n",
      "vqa_loss: 0.04417415335774422\n",
      "vqa_loss: 0.041961368173360825\n",
      "vqa_loss: 0.040035441517829895\n",
      "vqa_loss: 0.038344044238328934\n",
      "vqa_loss: 0.036841981112957\n",
      "vqa_loss: 0.03550305590033531\n",
      "vqa_loss: 0.034300729632377625\n",
      "vqa_loss: 0.03321187198162079\n",
      "vqa_loss: 0.03222278878092766\n",
      "vqa_loss: 0.03131791204214096\n",
      "vqa_loss: 0.03048744611442089\n",
      "vqa_loss: 0.029721679165959358\n",
      "vqa_loss: 0.029011331498622894\n",
      "vqa_loss: 0.028353417292237282\n",
      "vqa_loss: 0.027740398421883583\n",
      "vqa_loss: 0.027167702093720436\n",
      "vqa_loss: 0.026631342247128487\n",
      "vqa_loss: 0.026126477867364883\n",
      "vqa_loss: 0.025651244446635246\n",
      "vqa_loss: 0.025200916454195976\n",
      "vqa_loss: 0.024777544662356377\n",
      "vqa_loss: 0.02437400631606579\n",
      "vqa_loss: 0.02399163693189621\n",
      "vqa_loss: 0.023627594113349915\n",
      "vqa_loss: 0.02328105829656124\n",
      "vqa_loss: 0.02295001782476902\n",
      "vqa_loss: 0.022633293643593788\n",
      "vqa_loss: 0.02233053930103779\n",
      "vqa_loss: 0.022039497271180153\n",
      "vqa_loss: 0.021759461611509323\n",
      "vqa_loss: 0.02149271033704281\n",
      "vqa_loss: 0.021235186606645584\n",
      "vqa_loss: 0.020987382158637047\n",
      "vqa_loss: 0.02074703387916088\n",
      "vqa_loss: 0.020515933632850647\n",
      "vqa_loss: 0.020293014124035835\n",
      "vqa_loss: 0.02007744461297989\n",
      "vqa_loss: 0.019868988543748856\n",
      "vqa_loss: 0.019664550200104713\n",
      "vqa_loss: 0.019468897953629494\n",
      "vqa_loss: 0.01927858591079712\n",
      "vqa_loss: 0.019094200804829597\n",
      "vqa_loss: 0.018915634602308273\n",
      "vqa_loss: 0.01874145120382309\n",
      "vqa_loss: 0.018572017550468445\n",
      "vqa_loss: 0.018407568335533142\n",
      "vqa_loss: 0.018247630447149277\n",
      "vqa_loss: 0.018091848120093346\n",
      "vqa_loss: 0.01793723739683628\n",
      "vqa_loss: 0.017789172008633614\n",
      "vqa_loss: 0.017646094784140587\n",
      "vqa_loss: 0.01750515028834343\n",
      "vqa_loss: 0.017366692423820496\n",
      "vqa_loss: 0.01723203994333744\n",
      "vqa_loss: 0.01710059493780136\n",
      "vqa_loss: 0.01697259582579136\n",
      "vqa_loss: 0.016845181584358215\n",
      "vqa_loss: 0.016721809282898903\n",
      "vqa_loss: 0.01660236343741417\n",
      "vqa_loss: 0.016483742743730545\n",
      "vqa_loss: 0.016368450596928596\n",
      "vqa_loss: 0.016254466027021408\n",
      "vqa_loss: 0.016145355999469757\n",
      "vqa_loss: 0.01603504829108715\n",
      "vqa_loss: 0.01592854969203472\n",
      "vqa_loss: 0.015823116526007652\n",
      "vqa_loss: 0.01572137326002121\n",
      "vqa_loss: 0.01562045980244875\n",
      "vqa_loss: 0.015522398985922337\n",
      "vqa_loss: 0.015424811281263828\n",
      "vqa_loss: 0.015330078080296516\n",
      "vqa_loss: 0.01523593720048666\n",
      "vqa_loss: 0.01514393649995327\n",
      "vqa_loss: 0.015054318122565746\n",
      "vqa_loss: 0.014964216388761997\n",
      "vqa_loss: 0.014875542372465134\n",
      "vqa_loss: 0.014789126813411713\n",
      "vqa_loss: 0.014704855158925056\n",
      "vqa_loss: 0.014621175825595856\n",
      "vqa_loss: 0.01454035472124815\n",
      "vqa_loss: 0.014460244216024876\n",
      "vqa_loss: 0.014379297383129597\n",
      "vqa_loss: 0.014301087707281113\n",
      "vqa_loss: 0.014223829843103886\n",
      "vqa_loss: 0.014148715883493423\n",
      "vqa_loss: 0.014073953963816166\n",
      "vqa_loss: 0.014001576229929924\n",
      "vqa_loss: 0.013930028304457664\n",
      "vqa_loss: 0.013857406564056873\n",
      "vqa_loss: 0.013785973191261292\n",
      "vqa_loss: 0.013714775443077087\n",
      "vqa_loss: 0.013647274114191532\n",
      "vqa_loss: 0.01357976719737053\n",
      "vqa_loss: 0.013512857258319855\n",
      "vqa_loss: 0.013446062803268433\n",
      "vqa_loss: 0.013381650671362877\n",
      "vqa_loss: 0.013318906538188457\n",
      "vqa_loss: 0.01325556356459856\n",
      "vqa_loss: 0.013193292543292046\n",
      "vqa_loss: 0.013131018728017807\n",
      "vqa_loss: 0.013070054352283478\n",
      "vqa_loss: 0.013009686954319477\n",
      "vqa_loss: 0.012950627133250237\n",
      "vqa_loss: 0.012892400845885277\n",
      "vqa_loss: 0.012835722416639328\n",
      "vqa_loss: 0.012779400683939457\n",
      "vqa_loss: 0.01272295881062746\n",
      "vqa_loss: 0.012668303214013577\n",
      "vqa_loss: 0.012612813152372837\n",
      "vqa_loss: 0.012558035552501678\n",
      "vqa_loss: 0.012504209764301777\n",
      "vqa_loss: 0.012451933696866035\n",
      "vqa_loss: 0.012398941442370415\n",
      "vqa_loss: 0.012347735464572906\n",
      "vqa_loss: 0.012294742278754711\n",
      "vqa_loss: 0.012242819182574749\n",
      "vqa_loss: 0.012192564085125923\n",
      "vqa_loss: 0.012142783962190151\n",
      "vqa_loss: 0.012092885561287403\n",
      "vqa_loss: 0.012044535018503666\n",
      "vqa_loss: 0.011996421962976456\n",
      "vqa_loss: 0.011948545463383198\n",
      "vqa_loss: 0.011902696453034878\n",
      "vqa_loss: 0.01185624860227108\n",
      "vqa_loss: 0.011809802614152431\n",
      "vqa_loss: 0.011763829737901688\n",
      "vqa_loss: 0.011719406582415104\n",
      "vqa_loss: 0.011674865148961544\n",
      "vqa_loss: 0.011629128828644753\n",
      "vqa_loss: 0.011587089858949184\n",
      "vqa_loss: 0.011543498374521732\n",
      "vqa_loss: 0.011500979773700237\n",
      "vqa_loss: 0.011458341032266617\n",
      "vqa_loss: 0.011415582150220871\n",
      "vqa_loss: 0.011372466571629047\n",
      "vqa_loss: 0.011333282105624676\n",
      "vqa_loss: 0.011291354894638062\n",
      "vqa_loss: 0.01124919205904007\n",
      "vqa_loss: 0.011210124008357525\n",
      "vqa_loss: 0.01116974651813507\n",
      "vqa_loss: 0.011130680330097675\n",
      "vqa_loss: 0.011092565022408962\n",
      "vqa_loss: 0.011053616181015968\n",
      "vqa_loss: 0.011015620082616806\n",
      "vqa_loss: 0.010976193472743034\n",
      "vqa_loss: 0.010939506813883781\n",
      "vqa_loss: 0.010902820155024529\n",
      "vqa_loss: 0.010865421034395695\n",
      "vqa_loss: 0.01082825567573309\n",
      "vqa_loss: 0.010791448876261711\n",
      "vqa_loss: 0.010754880495369434\n",
      "vqa_loss: 0.010719622485339642\n",
      "vqa_loss: 0.010684482753276825\n",
      "vqa_loss: 0.010647912509739399\n",
      "vqa_loss: 0.010613368824124336\n",
      "vqa_loss: 0.010578705929219723\n",
      "vqa_loss: 0.010543445125222206\n",
      "vqa_loss: 0.010509616695344448\n",
      "vqa_loss: 0.010477693751454353\n",
      "vqa_loss: 0.010444936342537403\n",
      "vqa_loss: 0.01041253563016653\n",
      "vqa_loss: 0.010380254127085209\n",
      "vqa_loss: 0.010346421971917152\n",
      "vqa_loss: 0.010312828235328197\n",
      "vqa_loss: 0.010280903428792953\n",
      "vqa_loss: 0.010249574668705463\n",
      "vqa_loss: 0.010218006558716297\n",
      "vqa_loss: 0.010186675935983658\n",
      "vqa_loss: 0.010154631920158863\n",
      "vqa_loss: 0.010124015621840954\n",
      "vqa_loss: 0.01009506918489933\n",
      "vqa_loss: 0.010063857771456242\n",
      "vqa_loss: 0.010033836588263512\n",
      "vqa_loss: 0.010003459639847279\n",
      "vqa_loss: 0.009973915293812752\n",
      "vqa_loss: 0.009944370947778225\n",
      "vqa_loss: 0.009914826601743698\n",
      "vqa_loss: 0.009886474348604679\n",
      "vqa_loss: 0.009856929071247578\n",
      "vqa_loss: 0.009827743284404278\n",
      "vqa_loss: 0.009800223633646965\n",
      "vqa_loss: 0.009771632961928844\n",
      "vqa_loss: 0.009743635542690754\n",
      "vqa_loss: 0.009716711938381195\n",
      "vqa_loss: 0.009689192287623882\n",
      "vqa_loss: 0.009662388823926449\n",
      "vqa_loss: 0.009634033776819706\n",
      "vqa_loss: 0.009606512263417244\n",
      "vqa_loss: 0.009580064564943314\n",
      "vqa_loss: 0.009552663192152977\n",
      "vqa_loss: 0.029210662469267845\n",
      "vqa_loss: 0.02880636230111122\n",
      "vqa_loss: 0.02841613069176674\n",
      "vqa_loss: 0.028042009100317955\n",
      "vqa_loss: 0.02767935022711754\n",
      "vqa_loss: 0.027325546368956566\n",
      "vqa_loss: 0.026988467201590538\n",
      "vqa_loss: 0.026658467948436737\n",
      "vqa_loss: 0.02634163200855255\n",
      "vqa_loss: 0.02602902054786682\n",
      "vqa_loss: 0.025730066001415253\n",
      "vqa_loss: 0.0254374910145998\n",
      "vqa_loss: 0.025153446942567825\n",
      "vqa_loss: 0.02487746626138687\n",
      "vqa_loss: 0.024605853483080864\n",
      "vqa_loss: 0.024345291778445244\n",
      "vqa_loss: 0.024091120809316635\n",
      "vqa_loss: 0.023840975016355515\n",
      "vqa_loss: 0.02359830215573311\n",
      "vqa_loss: 0.02336108684539795\n",
      "vqa_loss: 0.023131947964429855\n",
      "vqa_loss: 0.022907909005880356\n",
      "vqa_loss: 0.02268598973751068\n",
      "vqa_loss: 0.0224692914634943\n",
      "vqa_loss: 0.02225913107395172\n",
      "vqa_loss: 0.02205466665327549\n",
      "vqa_loss: 0.02185424230992794\n",
      "vqa_loss: 0.021655581891536713\n",
      "vqa_loss: 0.02146119624376297\n",
      "vqa_loss: 0.021274544298648834\n",
      "vqa_loss: 0.021091455593705177\n",
      "vqa_loss: 0.02091013826429844\n",
      "vqa_loss: 0.020732024684548378\n",
      "vqa_loss: 0.020557712763547897\n",
      "vqa_loss: 0.02038910612463951\n",
      "vqa_loss: 0.020220376551151276\n",
      "vqa_loss: 0.02005651779472828\n",
      "vqa_loss: 0.019895395264029503\n",
      "vqa_loss: 0.019739026203751564\n",
      "vqa_loss: 0.019584672525525093\n",
      "vqa_loss: 0.019431741908192635\n",
      "vqa_loss: 0.01928214356303215\n",
      "vqa_loss: 0.019136467948555946\n",
      "vqa_loss: 0.01899018883705139\n",
      "vqa_loss: 0.01884998008608818\n",
      "vqa_loss: 0.018711911514401436\n",
      "vqa_loss: 0.018573597073554993\n",
      "vqa_loss: 0.018440403044223785\n",
      "vqa_loss: 0.018309228122234344\n",
      "vqa_loss: 0.01817971281707287\n",
      "vqa_loss: 0.018052460625767708\n",
      "vqa_loss: 0.017924608662724495\n",
      "vqa_loss: 0.01780175231397152\n",
      "vqa_loss: 0.017680322751402855\n",
      "vqa_loss: 0.017562588676810265\n",
      "vqa_loss: 0.01744496449828148\n",
      "vqa_loss: 0.017328407615423203\n",
      "vqa_loss: 0.01721268519759178\n",
      "vqa_loss: 0.017098624259233475\n",
      "vqa_loss: 0.016986705362796783\n",
      "vqa_loss: 0.016878001391887665\n",
      "vqa_loss: 0.016770843416452408\n",
      "vqa_loss: 0.016663921996951103\n",
      "vqa_loss: 0.01655985414981842\n",
      "vqa_loss: 0.01645781099796295\n",
      "vqa_loss: 0.01635648123919964\n",
      "vqa_loss: 0.01625276543200016\n",
      "vqa_loss: 0.016156790778040886\n",
      "vqa_loss: 0.016060935333371162\n",
      "vqa_loss: 0.015964483842253685\n",
      "vqa_loss: 0.015866711735725403\n",
      "vqa_loss: 0.015774544328451157\n",
      "vqa_loss: 0.015682609751820564\n",
      "vqa_loss: 0.015589604154229164\n",
      "vqa_loss: 0.015500646084547043\n",
      "vqa_loss: 0.015411567874252796\n",
      "vqa_loss: 0.01532236859202385\n",
      "vqa_loss: 0.015234715305268764\n",
      "vqa_loss: 0.015150635503232479\n",
      "vqa_loss: 0.015067627653479576\n",
      "vqa_loss: 0.014984498731791973\n",
      "vqa_loss: 0.014904230833053589\n",
      "vqa_loss: 0.01482515037059784\n",
      "vqa_loss: 0.014743090607225895\n",
      "vqa_loss: 0.01466388814151287\n",
      "vqa_loss: 0.014584802091121674\n",
      "vqa_loss: 0.01450857799500227\n",
      "vqa_loss: 0.01443199347704649\n",
      "vqa_loss: 0.014356243424117565\n",
      "vqa_loss: 0.014282993972301483\n",
      "vqa_loss: 0.014208788052201271\n",
      "vqa_loss: 0.014132794924080372\n",
      "vqa_loss: 0.014059780165553093\n",
      "vqa_loss: 0.013990817591547966\n",
      "vqa_loss: 0.013918992131948471\n",
      "vqa_loss: 0.01384990755468607\n",
      "vqa_loss: 0.0137816546484828\n",
      "vqa_loss: 0.013711496256291866\n",
      "vqa_loss: 0.013645744882524014\n",
      "vqa_loss: 0.013578442856669426\n",
      "vqa_loss: 0.013513642363250256\n",
      "vqa_loss: 0.013445981778204441\n",
      "vqa_loss: 0.013383327051997185\n",
      "vqa_loss: 0.013319715857505798\n",
      "vqa_loss: 0.013256224803626537\n",
      "vqa_loss: 0.01319237519055605\n",
      "vqa_loss: 0.013132816180586815\n",
      "vqa_loss: 0.013069679029285908\n",
      "vqa_loss: 0.013008089736104012\n",
      "vqa_loss: 0.012950316071510315\n",
      "vqa_loss: 0.012890157289803028\n",
      "vqa_loss: 0.01283202413469553\n",
      "vqa_loss: 0.012775318697094917\n",
      "vqa_loss: 0.012716828845441341\n",
      "vqa_loss: 0.01265809591859579\n",
      "vqa_loss: 0.012601747177541256\n",
      "vqa_loss: 0.012545160949230194\n",
      "vqa_loss: 0.01248940546065569\n",
      "vqa_loss: 0.012435559183359146\n",
      "vqa_loss: 0.012383618392050266\n",
      "vqa_loss: 0.01232834067195654\n",
      "vqa_loss: 0.012275207787752151\n",
      "vqa_loss: 0.01222207210958004\n",
      "vqa_loss: 0.012167506851255894\n",
      "vqa_loss: 0.012118305079638958\n",
      "vqa_loss: 0.012068746611475945\n",
      "vqa_loss: 0.012017516419291496\n",
      "vqa_loss: 0.011967598460614681\n",
      "vqa_loss: 0.011916248127818108\n",
      "vqa_loss: 0.01186728198081255\n",
      "vqa_loss: 0.011816885322332382\n",
      "vqa_loss: 0.011770781129598618\n",
      "vqa_loss: 0.011721096932888031\n",
      "vqa_loss: 0.011673441156744957\n",
      "vqa_loss: 0.011626380495727062\n",
      "vqa_loss: 0.011580273509025574\n",
      "vqa_loss: 0.011534045450389385\n",
      "vqa_loss: 0.01148901041597128\n",
      "vqa_loss: 0.011443140916526318\n",
      "vqa_loss: 0.011399056762456894\n",
      "vqa_loss: 0.01135342475026846\n",
      "vqa_loss: 0.011308028362691402\n",
      "vqa_loss: 0.011262991465628147\n",
      "vqa_loss: 0.01122081559151411\n",
      "vqa_loss: 0.011178399436175823\n",
      "vqa_loss: 0.011135387234389782\n",
      "vqa_loss: 0.01109297014772892\n",
      "vqa_loss: 0.011051864363253117\n",
      "vqa_loss: 0.011009925045073032\n",
      "vqa_loss: 0.010967744514346123\n",
      "vqa_loss: 0.01092735305428505\n",
      "vqa_loss: 0.01088672410696745\n",
      "vqa_loss: 0.010845736600458622\n",
      "vqa_loss: 0.010804747231304646\n",
      "vqa_loss: 0.010765546932816505\n",
      "vqa_loss: 0.010727301239967346\n",
      "vqa_loss: 0.010687503963708878\n",
      "vqa_loss: 0.010649733245372772\n",
      "vqa_loss: 0.010612200945615768\n",
      "vqa_loss: 0.010575024411082268\n",
      "vqa_loss: 0.010536775924265385\n",
      "vqa_loss: 0.010498170740902424\n",
      "vqa_loss: 0.010460040532052517\n",
      "vqa_loss: 0.010424653068184853\n",
      "vqa_loss: 0.0103871189057827\n",
      "vqa_loss: 0.010347438976168633\n",
      "vqa_loss: 0.010313480161130428\n",
      "vqa_loss: 0.010278687812387943\n",
      "vqa_loss: 0.010241747833788395\n",
      "vqa_loss: 0.010206597857177258\n",
      "vqa_loss: 0.010171565227210522\n",
      "vqa_loss: 0.010137367062270641\n",
      "vqa_loss: 0.010101976804435253\n",
      "vqa_loss: 0.01006658561527729\n",
      "vqa_loss: 0.010032863356173038\n",
      "vqa_loss: 0.009999498724937439\n",
      "vqa_loss: 0.009965896606445312\n",
      "vqa_loss: 0.009933367371559143\n",
      "vqa_loss: 0.009899405762553215\n",
      "vqa_loss: 0.009865683503448963\n",
      "vqa_loss: 0.009833749383687973\n",
      "vqa_loss: 0.00980133842676878\n",
      "vqa_loss: 0.009768686257302761\n",
      "vqa_loss: 0.00973651371896267\n",
      "vqa_loss: 0.009704099968075752\n",
      "vqa_loss: 0.00967335794121027\n",
      "vqa_loss: 0.009643328376114368\n",
      "vqa_loss: 0.00961246620863676\n",
      "vqa_loss: 0.00958172231912613\n",
      "vqa_loss: 0.009550740011036396\n",
      "vqa_loss: 0.009519875049591064\n",
      "vqa_loss: 0.009488893672823906\n",
      "vqa_loss: 0.009458625689148903\n",
      "vqa_loss: 0.00943074282258749\n",
      "vqa_loss: 0.009398924186825752\n",
      "vqa_loss: 0.009370445273816586\n",
      "vqa_loss: 0.009342322126030922\n",
      "vqa_loss: 0.009313245303928852\n",
      "vqa_loss: 0.009284883737564087\n",
      "vqa_loss: 0.00925771426409483\n",
      "\n",
      "2022-06-03 11:10:06,238 - INFO - allennlp.models.archival - loading archive file /brtx/605-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_double//ckpt/model.tar.gz\n",
      "2022-06-03 11:10:06,239 - INFO - allennlp.models.archival - extracting archive file /brtx/605-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_double//ckpt/model.tar.gz to temp dir /tmp/tmp3q3eng40\n",
      "Some weights of the model checkpoint at /brtx/605-nvme1/estengel/annotator_uncertainty/models/finetune_vilt_pytorch/ were not used when initializing ViltModel: ['classifier.1.weight', 'classifier.3.bias', 'classifier.1.bias', 'classifier.0.weight', 'classifier.3.weight', 'classifier.0.bias']\n",
      "- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-06-03 11:10:12,938 - INFO - allennlp.data.dataset_readers.vision_reader - Processing images on device 0\n",
      "2022-06-03 11:10:12,938 - INFO - allennlp.data.dataset_readers.vision_reader - Discovering images ...\n",
      "Discovering jpg images: 123287it [00:18, 6709.05it/s]  \n",
      "Discovering png images: 0it [00:01, ?it/s]\n",
      "2022-06-03 11:10:33,289 - INFO - allennlp.data.dataset_readers.vision_reader - Done discovering images\n",
      "Some weights of the model checkpoint at /brtx/605-nvme1/estengel/annotator_uncertainty/models/finetune_vilt_pytorch/ were not used when initializing ViltModel: ['classifier.1.weight', 'classifier.3.bias', 'classifier.1.bias', 'classifier.0.weight', 'classifier.3.weight', 'classifier.0.bias']\n",
      "- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-06-03 11:10:34,776 - INFO - allennlp.data.dataset_readers.vision_reader - Processing images on device 0\n",
      "2022-06-03 11:10:34,776 - INFO - allennlp.data.dataset_readers.vision_reader - Discovering images ...\n",
      "Discovering jpg images: 123287it [00:02, 55227.32it/s] \n",
      "Discovering png images: 0it [00:01, ?it/s]\n",
      "2022-06-03 11:10:38,987 - INFO - allennlp.data.dataset_readers.vision_reader - Done discovering images\n",
      "2022-06-03 11:10:39,154 - INFO - allennlp.data.vocabulary - Loading token dictionary from /tmp/tmp3q3eng40/vocabulary.\n",
      "Some weights of the model checkpoint at /brtx/605-nvme1/estengel/annotator_uncertainty/models/finetune_vilt_pytorch/ were not used when initializing ViltModel: ['classifier.1.weight', 'classifier.3.bias', 'classifier.1.bias', 'classifier.0.weight', 'classifier.3.weight', 'classifier.0.bias']\n",
      "- This IS expected if you are initializing ViltModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViltModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-06-03 11:10:45,320 - INFO - allennlp.models.archival - removing temporary unarchived model dir at /tmp/tmp3q3eng40\n",
      "2022-06-03 11:10:45,396 - INFO - allennlp.common.checks - Pytorch version: 1.10.1+cu102\n",
      "2022-06-03 11:10:45,397 - INFO - allennlp.commands.min_gen - Reading evaluation data from /brtx/603-nvme2/estengel/annotator_uncertainty/vqa/dev_from_mturk_small/\n",
      "loading instances: 0it [00:00, ?it/s]2022-06-03 11:10:45,402 - WARNING - allennlp.data.fields.label_field - Your label namespace was 'answers'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.\n",
      "loading instances: 4it [00:00, 760.25it/s]\n",
      "2022-06-03 11:10:45,425 - INFO - allennlp.training.util - Iterating over dataset\n",
      "0it [00:00, ?it/s]/home/estengel/annotator_uncertainty/models/allennlp/nn/beam_search.py:746: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  backpointer = restricted_beam_indices // self.per_node_beam_size\n",
      "precision: 0.00, recall: 0.00, fscore: 0.00, vqa_score: 0.08, speaker_bleu_0: 1.00, loss: 0.19 ||: : 4it [00:42, 10.74s/it]\n",
      "2022-06-03 11:11:28,390 - INFO - allennlp.commands.min_gen - Finished evaluating.\n",
      "\n",
      "============================\n",
      "CHECKPOINT: /brtx/605-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_double/ LR: 0.05: STEPS: 200\n",
      "{'debug_tokens': ['What letters are attached to the pole?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000276311.jpg'], 'debug_answer': [{'white': 1}], 'speaker_outputs': [['what what is for the woman ?', 'what is this for the woman ?', 'what is this in the ice cream ?', 'what what is this for the woman ?', 'what is this in the woman is preparing with the ice cream ?']], 'original_loss': 6.155438423156738, 'question_id': '276311000_4', 'final_loss': 0.6555130481719971}\n",
      "{'debug_tokens': ['What railway does this train run along?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000337875.jpg'], 'debug_answer': [{'grand canyon': 1}], 'speaker_outputs': [['what is the pink and black hat ?', 'what is the pink and white hat ?', 'what is the pink object in the picture ?', 'what is the pink and black figure ?', 'what is the pink and black ball ?']], 'original_loss': 0.1611129641532898, 'question_id': '337875005_3', 'final_loss': 0.009689426980912685}\n",
      "{'debug_tokens': ['What is on the computer screen?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000420523.jpg'], 'debug_answer': [{'banana and strawberry': 1}], 'speaker_outputs': [[\"what ' s or @@UNKNOWN@@ of the oil ?\", \"what ' s or @@UNKNOWN@@ of the oil on the table ?\", \"what ' s or @@UNKNOWN@@ of the @@UNKNOWN@@ on the surface ?\", \"what ' s or @@UNKNOWN@@ of the oil on the surface ?\", \"what ' s or @@UNKNOWN@@ of the @@UNKNOWN@@ on the surface of the monitor ?\"]], 'original_loss': 1.0492568016052246, 'question_id': '420523002_5', 'final_loss': 0.009552663192152977}\n",
      "{'debug_tokens': ['What are the pizzas on?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000172121.jpg'], 'debug_answer': [{'pepperoni and cheese': 1}], 'speaker_outputs': [['what is the word \" @@UNKNOWN@@ \" ?', 'what is the \" @@UNKNOWN@@ \" ?', 'what is the word \" @@UNKNOWN@@ \" on the desk ?', 'what is the word end of the \" ?', 'what is the \" @@UNKNOWN@@ \" on the desk ?']], 'original_loss': 3.7832698822021484, 'question_id': '172121017_6', 'final_loss': 0.00925771426409483}\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "# checkpoints = [,\n",
    "#               \"/brtx/605-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_double/\",\n",
    "#               \"/brtx/604-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_2_layer_copy_sl/\"]\n",
    "# checkpoints = [\"/brtx/606-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_weighted_0.5_1.0\"]\n",
    "checkpoints = [\"/brtx/605-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_double/\"]\n",
    "for checkpoint in checkpoints:\n",
    "    # for lr in [0.001, 0.01, 0.05, 0.1]:\n",
    "    #     for steps in [50, 100, 200, 400]:\n",
    "    for lr in [0.05]:\n",
    "        for steps in [200]:\n",
    "            out, err, output_data = min_gen(checkpoint=checkpoint,\n",
    "                                            steps=steps, lr=lr, device=7)\n",
    "\n",
    "            print(err.decode(\"utf-8\"))\n",
    "            print(f\"============================\")\n",
    "            print(f\"CHECKPOINT: {checkpoint} LR: {lr}: STEPS: {steps}\")\n",
    "            for i in range(len(output_data)):\n",
    "                print(output_data[i])\n",
    "            print(f\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = [\"/brtx/606-nvme1/estengel/annotator_uncertainty/models/vilt_bce_ce_weighted_1.0_0.5\"]\n",
    "for checkpoint in checkpoints:\n",
    "    for lr in [0.01, 0.05, 0.1]:\n",
    "        for steps in [200, 400]:\n",
    "            out, err, output_data = min_gen(checkpoint=checkpoint,\n",
    "                                            steps=steps, lr=lr, device=7)\n",
    "            print(f\"============================\")\n",
    "            print(f\"CHECKPOINT: {checkpoint} LR: {lr}: STEPS: {steps}\")\n",
    "            for i in range(len(output_data)):\n",
    "                print(output_data[i])\n",
    "            print(f\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "CHECKPOINT: /brtx/602-nvme1/estengel/annotator_uncertainty/models/vilt_mlm/ LR: 0.01: STEPS: 2000\n",
      "{'debug_tokens': ['What letters are attached to the pole?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000276311.jpg'], 'debug_answer': [{'white': 1}], 'speaker_outputs': [['what color are the letters ?', 'what letters are on the sign ?', 'what color are the signs ?', 'what letters are on the wall ?', 'what color are on the sign ?']], 'original_loss': 6.140600681304932, 'question_id': '276311000_4', 'final_loss': 0.13005851209163666}\n",
      "{'debug_tokens': ['What railway does this train run along?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000337875.jpg'], 'debug_answer': [{'grand canyon': 1}], 'speaker_outputs': [['what railway does the train show ?', 'what railway does the train run on ?', 'what railway does this train run on ?', 'what railway station does this train show ?', 'what railway station does this train run ?']], 'original_loss': 0.08142612874507904, 'question_id': '337875005_3', 'final_loss': 0.014521575532853603}\n",
      "{'debug_tokens': ['What is on the computer screen?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000420523.jpg'], 'debug_answer': [{'banana and strawberry': 1}], 'speaker_outputs': [['what is on the screen ?', 'what is on the computer screen ?', 'what is on the monitor ?', 'what is on the screen screen ?', 'what is on the monitor screen ?']], 'original_loss': 0.23518836498260498, 'question_id': '420523002_5', 'final_loss': 0.012846963480114937}\n",
      "{'debug_tokens': ['What are the pizzas on?'], 'debug_images': ['/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/balanced_real/COCO_train2014_000000172121.jpg'], 'debug_answer': [{'pepperoni and cheese': 1}], 'speaker_outputs': [['what are the pizza ##s on ?', 'what are the pizza ##s eating ?', 'what are the pizza ##s sitting on ?', 'what are the pizza on ?', 'what are the computers on ?']], 'original_loss': 0.3659961223602295, 'question_id': '172121017_6', 'final_loss': 0.011005766689777374}\n",
      "============================\n"
     ]
    }
   ],
   "source": [
    "checkpoints = [\"/brtx/602-nvme1/estengel/annotator_uncertainty/models/vilt_mlm/\"]\n",
    "for checkpoint in checkpoints:\n",
    "    for lr in [0.01, 0.05, 0.1]:\n",
    "        for steps in [200, 400]:\n",
    "    # for lr in [0.01]:\n",
    "        # for steps in [2000]:\n",
    "            out, err, output_data = min_gen(checkpoint=checkpoint,\n",
    "                                            steps=steps, lr=lr, device=7)\n",
    "            print(f\"============================\")\n",
    "            print(f\"CHECKPOINT: {checkpoint} LR: {lr}: STEPS: {steps}\")\n",
    "            for i in range(len(output_data)):\n",
    "                print(output_data[i])\n",
    "            print(f\"============================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a71203a0affc7207a6dfd2214cf786ae35ec96fb0be57392a936686760a696bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
