{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import json \n",
    "import re \n",
    "import pathlib \n",
    "import numpy as np\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pred_file(path):\n",
    "    data_by_qid = {}\n",
    "    with open(path) as f1:\n",
    "        for line in f1:\n",
    "            batch = json.loads(line)\n",
    "            qids = batch['question_id']\n",
    "            preds = batch['speaker_utterances'][0]\n",
    "            for qid, pred in zip(qids, preds):\n",
    "                pred = re.sub(\"<[^>]*?>\", \"\", pred)\n",
    "                pred = pred.strip()\n",
    "                data_by_qid[qid] = pred\n",
    "    return data_by_qid\n",
    "\n",
    "\n",
    "def read_csv_data(path):\n",
    "    data_by_qid = {}\n",
    "    with open(path) as f1:\n",
    "        reader = csv.DictReader(f1)\n",
    "        for row in reader:\n",
    "            qid = row['Input.question_id']\n",
    "            for i, sent in enumerate(row['Answer.answer_questions']):\n",
    "                new_qid = f\"{qid}_{i}\"\n",
    "                data_by_qid[new_qid] = {\"sent\": sent, \"img_url\": row['Input.imgUrl']}\n",
    "    return data_by_qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_questions = read_pred_file('/brtx/602-nvme1/estengel/annotator_uncertainty/models/img2q_t5_base_no_limit/output/test_set_predictions_forced.jsonl') \n",
    "ann_questions = json.load(open(\"/home/estengel/annotator_uncertainty/jimena_work/cleaned_data/csv/test_set/questions.json\"))['questions']\n",
    "ann_annotations = json.load(open(\"/home/estengel/annotator_uncertainty/jimena_work/cleaned_data/csv/test_set/annotations.json\"))['annotations']\n",
    "ann_img_ids = read_csv_data(\"/home/estengel/annotator_uncertainty/jimena_work/cleaned_data/csv/test_set/consolidate_data_repeat_all_data.csv\")\n",
    "original_questions = json.load(open(\"/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/v2_OpenEnded_mscoco_train2014_questions.json\"))['questions']\n",
    "\n",
    "ann_questions = {q['question_id']: q for q in ann_questions}\n",
    "ann_annotations = {q['question_id']: q for q in ann_annotations}\n",
    "original_questions = {q['question_id']: q for q in original_questions}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "# sample 100 question ids \n",
    "all_qids = sorted(list(pred_questions.keys()))\n",
    "chosen = np.random.choice(all_qids, size=100, replace=False)\n",
    "\n",
    "data = []\n",
    "# get the pred, annotator, original, and random questions for each qid \n",
    "for i, qid in enumerate(chosen):\n",
    "\n",
    "    pred = pred_questions[qid]\n",
    "    ann = ann_questions[qid]['new_question']\n",
    "    img_url = ann_img_ids[qid]['img_url']\n",
    "    answer = ann_annotations[qid]['answers'][0]['answer']\n",
    "    orig_qid = int(qid.split(\"_\")[0])\n",
    "    orig = original_questions[orig_qid]['question']\n",
    "\n",
    "    # choose random qid and get original question for distractor \n",
    "    random_qid = np.random.choice(all_qids, size=1, replace=False)[0]\n",
    "    # just in case it somehow chooses the same \n",
    "    while random_qid  == qid: \n",
    "        random_qid = np.random.choice(all_qids, size=1, replace=False)[0]\n",
    "    rand_orig_qid = int(random_qid.split(\"_\")[0])\n",
    "    rand_question = original_questions[rand_orig_qid]['question']\n",
    "\n",
    "    datapoint = {\"qid\": qid, \"pred_question\": pred, \"ann_question\": ann, \"orig_question\": orig, \"rand_question\": rand_question, \"answer\": answer, \"img_url\": img_url}\n",
    "    data.append(datapoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomize data for HIT\n",
    "np.random.seed(12)\n",
    "\n",
    "data_for_hit = []\n",
    "for datapoint in data: \n",
    "    questions = [\"pred_question\", \"ann_question\", \"orig_question\", \"rand_question\"]\n",
    "    # questions = [(i,x) for i,x in enumerate(questions)]\n",
    "    # pick a random order \n",
    "    np.random.shuffle(questions)\n",
    "    # indices, questions = zip(*questions)\n",
    "    questions = [(i,x) for i,x in enumerate(questions)]\n",
    "\n",
    "    hit_datapoint = {\"qid\": datapoint[\"qid\"], \"img_url\": json.loads(datapoint[\"img_url\"]), \"answer\": datapoint[\"answer\"], \"indices_and_questions\": questions} \n",
    "    for i in range(len(questions)):\n",
    "        hit_datapoint[f\"question_{i}\"] = datapoint[questions[i][1]]\n",
    "    data_for_hit.append(hit_datapoint)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': '238290005_6', 'img_url': 'https://cs.jhu.edu/~esteng/images_for_hit/COCO_train2014_000000238290.jpg', 'answer': 'plane', 'indices_and_questions': [(0, 'pred_question'), (1, 'orig_question'), (2, 'rand_question'), (3, 'ann_question')], 'question_0': 'What is in the air? source', 'question_1': 'Besides the sun, what is the other light source in this scene?', 'question_2': 'What time of day is this?', 'question_3': 'Besides the sun, what could be the other sky light source in this scene?'}\n",
      "{'qid': '238290005_6', 'pred_question': 'What is in the air? source', 'ann_question': 'Besides the sun, what could be the other sky light source in this scene?', 'orig_question': 'Besides the sun, what is the other light source in this scene?', 'rand_question': 'What time of day is this?', 'answer': 'plane', 'img_url': '\"https://cs.jhu.edu/~esteng/images_for_hit/COCO_train2014_000000238290.jpg\"'}\n"
     ]
    }
   ],
   "source": [
    "print(data_for_hit[4])\n",
    "print(data[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_keys = ['indices_and_questions']\n",
    "\n",
    "with open(\"../eval_hit/csvs/data_for_hit.csv\", \"w\") as f1:\n",
    "    writer = csv.DictWriter(f1, fieldnames=data_for_hit[0].keys())\n",
    "    writer.writeheader()\n",
    "    for row in data_for_hit:\n",
    "        row = {k: json.dumps(v) if v in json_keys else v for k, v in row.items() }\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a71203a0affc7207a6dfd2214cf786ae35ec96fb0be57392a936686760a696bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
