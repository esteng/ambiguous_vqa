{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from collections import defaultdict\n",
    "import json \n",
    "import pathlib \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations = json.load(open(\"/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/dev_from_mturk/annotations.json\"))['annotations']\n",
    "# questions = json.load(open(\"/brtx/603-nvme2/estengel/annotator_uncertainty/vqa/dev_from_mturk/questions.json\"))['questions']\n",
    "\n",
    "annotations = json.load(open(\"/home/estengel/annotator_uncertainty/jimena_work/cleaned_data/csv/test_set/annotations.json\"))['annotations']\n",
    "questions = json.load(open(\"/home/estengel/annotator_uncertainty/jimena_work/cleaned_data/csv/test_set/questions.json\"))['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS, MeanShift, estimate_bandwidth\n",
    "from sklearn.mixture import GaussianMixture, BayesianGaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "class BayesianGMMCluster:\n",
    "    def fit(self, vectors):\n",
    "        gmm_wrapper = BayesianGaussianMixture(n_components = vectors.shape[0])\n",
    "        gmm_wrapper.fit(vectors)\n",
    "        weights = np.round(gmm_wrapper.weights_, 1)\n",
    "        probs = gmm_wrapper.predict_proba(vectors)\n",
    "        non_zero_weights = weights > 0\n",
    "        non_zero_weights = non_zero_weights.reshape(-1, 1)\n",
    "        non_zero_weights = np.tile(non_zero_weights, reps=probs.shape[1])\n",
    "        probs *= non_zero_weights\n",
    "        \n",
    "        labels = np.argmax(probs, axis=1)\n",
    "        # print(scores_data)\n",
    "        # score_array = np.array(scores_data)\n",
    "        # min_row = np.argmin(score_array)\n",
    "        self.labels_ = labels\n",
    "        # sys.exit() \n",
    "\n",
    "class GMMCluster:\n",
    "    def __init__(self, use_aic=True):\n",
    "        self.gmm_wrapper_dict = {k: GaussianMixture(n_components=k, random_state=12, n_init=2) for k in range(0, 10)}\n",
    "        self.use_aic = use_aic\n",
    "\n",
    "    def fit(self, vectors):\n",
    "        scores_data = []\n",
    "        assignments = []\n",
    "        all_labels = []\n",
    "        for k in tqdm(range(1, vectors.shape[0])):\n",
    "            gmm_wrapper = self.gmm_wrapper_dict[k]    \n",
    "            gmm_wrapper.fit(vectors) \n",
    "            if self.use_aic:\n",
    "                aic = gmm_wrapper.aic(vectors)\n",
    "                n_params = gmm_wrapper._n_parameters()\n",
    "                if vectors.shape[0] - n_params - 1 == 0:\n",
    "                    aicc = np.inf\n",
    "                else:\n",
    "\n",
    "                    aicc = (2 * n_params**2 + 2*n_params)/(vectors.shape[0] - n_params - 1) + aic\n",
    "                scores_data.append(aicc)\n",
    "            else:\n",
    "                scores_data.append(gmm_wrapper.bic(vectors))\n",
    "\n",
    "            labels = gmm_wrapper.predict(vectors)\n",
    "            all_labels.append(labels)\n",
    "        score_array = np.array(scores_data)\n",
    "        min_row = np.argmin(score_array)\n",
    "        self.labels_ = all_labels[min_row]\n",
    "\n",
    "\n",
    "class KMeansCluster:\n",
    "    def __init__(self, penalty_factor=3.0):\n",
    "        self.kmeans_wrapper_dict = {k: KMeans(n_clusters=k, random_state=12) for k in range(0, 10)}\n",
    "        self.penalty_factor = penalty_factor\n",
    "        self.labels_ = None\n",
    "\n",
    "    def fit(self, vectors):\n",
    "        scores_data = []\n",
    "        assignments = []\n",
    "        all_labels = []\n",
    "        for k in range(2, vectors.shape[0]):\n",
    "            kmeans_wrapper = self.kmeans_wrapper_dict[k]    \n",
    "            # run kmeans\n",
    "            kmeans = kmeans_wrapper.fit(vectors) \n",
    "            all_labels.append(kmeans.labels_)\n",
    "            centers = kmeans.predict(vectors) \n",
    "            num_centers = len(set(centers))\n",
    "            # intertia is sum of squared distances of samples to their closest cluster center \n",
    "            inertia = kmeans.inertia_\n",
    "            # penalty depends on how many centers you used compared to how many examples you have \n",
    "            penalty = self.penalty_factor * (num_centers-1)/ (vectors.shape[0]-1) \n",
    "            # we want balanced clusters\n",
    "            avg = int(vectors.shape[0] / num_centers)\n",
    "            num_per_center = {c_name: sum([1 for c in centers if c == c_name]) for c_name in centers}\n",
    "            diffs = [abs(avg - num_per_center[c]) for c in centers]\n",
    "            difference_penalty = sum(diffs)\n",
    "            score = inertia + penalty + difference_penalty \n",
    "\n",
    "            # score = kmeans.inertia_ + num_centers ** penalty_factor\n",
    "            # high intertia means dispersed, low means fits well to the number of clusters\n",
    "            scores_data.append((score, inertia, penalty, difference_penalty, num_centers))\n",
    "\n",
    "            assignments.append(centers)\n",
    "            # if you hit zero inertia, no sense in going further \n",
    "            if kmeans.inertia_ < 1e-16:\n",
    "                break\n",
    "\n",
    "        score_array = np.array(scores_data)\n",
    "        min_row = np.argmin(score_array[:,0])\n",
    "        self.labels_ = all_labels[min_row]\n",
    "\n",
    "\n",
    "# get the clusters from annotations \n",
    "def get_annotator_clusters(questions, annotations): \n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters_by_qid = {}\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        clusters = defaultdict(list)\n",
    "        for quest, ann in list_of_qas:\n",
    "            rewritten = quest['new_question']\n",
    "            answer = ann['answers'][0]['answer']\n",
    "            answer_id = ann['answers'][0]['mturk_id']\n",
    "            cluster_dict = {\"answer\": answer, \"id\": answer_id} \n",
    "            clusters[rewritten].append(cluster_dict)\n",
    "        clusters_by_qid[qid] = clusters\n",
    "    return clusters_by_qid\n",
    "\n",
    "# get the clusters from kmeans preprocessing\n",
    "def get_preprocessed_clusters(questions, annotations): \n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters_by_qid = {}\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        clusters = defaultdict(list)\n",
    "        for quest, ann in list_of_qas:\n",
    "            answer = ann['answers'][0]['answer']\n",
    "            answer_id = ann['answers'][0]['mturk_id']\n",
    "            id_key, answer_id_suffix = answer_id.split(\".\")\n",
    "            cluster_dict = {\"answer\": answer, \"id\": answer_id} \n",
    "            clusters[id_key].append(cluster_dict)\n",
    "        clusters_by_qid[qid] = clusters\n",
    "    return clusters_by_qid\n",
    "\n",
    "\n",
    "\n",
    "def cluster_vectors(vectors, cluster_method='optics', do_pca=False, pca_comp_max=2):\n",
    "    vidxs = [x[1] for x in vectors]\n",
    "    vectors = np.vstack([x[0] for x in vectors]).reshape(len(vectors), -1)\n",
    "    if do_pca:\n",
    "        from sklearn.decomposition import PCA\n",
    "        comp = min(vectors.shape[0], pca_comp_max)\n",
    "        pca = PCA(n_components=comp)\n",
    "        vectors = pca.fit_transform(vectors)\n",
    "\n",
    "    if cluster_method == \"optics\":\n",
    "        clust = OPTICS(min_samples=2, metric='euclidean')\n",
    "    elif cluster_method == \"mean_shift\":\n",
    "        bw = estimate_bandwidth(vectors, quantile=0.5)\n",
    "        if bw < 0.0001:\n",
    "            bw = 0.0001\n",
    "        clust = MeanShift(bandwidth=bw)\n",
    "    elif cluster_method == \"kmeans\": \n",
    "        clust = KMeansCluster(penalty_factor=52.0)\n",
    "    elif cluster_method == \"gmm\": \n",
    "        clust = GMMCluster(use_aic=True)\n",
    "    elif cluster_method == \"bayes_gmm\": \n",
    "        clust = BayesianGMMCluster()\n",
    "\n",
    "    else:\n",
    "        raise AssertionError(\"Unknown cluster method\")\n",
    "    clust.fit(vectors)\n",
    "    clusters = defaultdict(list)\n",
    "    max_label = max(clust.labels_) + 1\n",
    "    for i, vidx in enumerate(vidxs):\n",
    "        label = clust.labels_[i]\n",
    "        if label == -1:\n",
    "            label = max_label\n",
    "        clusters[label].append(vidx)\n",
    "    return clusters \n",
    "\n",
    "# get the clusters from predictions \n",
    "def get_prediction_clusters(questions, annotations, save_dir, cluster_method='optics', do_pca=False, pca_comp_max=2):\n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    vectors_by_qid = defaultdict(list)\n",
    "    answers_by_qid = defaultdict(list)\n",
    "\n",
    "    # print(anns_by_qid)\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        image_id = list_of_qas[0][0]['image_id']\n",
    "        for quest, ann in list_of_qas:\n",
    "            qid, i = quest['question_id'].split(\"_\")\n",
    "            path = pathlib.Path(save_dir).joinpath(f\"{image_id}_{qid}_{i}_0.pt\")\n",
    "            vector = torch.load(path, map_location=torch.device('cpu')).detach().numpy()\n",
    "            answer = ann['answers'][0]['answer']\n",
    "            answer_id = ann['answers'][0]['mturk_id']\n",
    "            vectors_by_qid[qid].append((vector, int(i)))\n",
    "            answers_by_qid[qid].append({\"answer\": answer, \"id\": answer_id})\n",
    "\n",
    "    clusters_by_qid = {}\n",
    "    for qid, vectors in vectors_by_qid.items():\n",
    "        clusters = cluster_vectors(vectors, cluster_method=cluster_method, do_pca=do_pca, pca_comp_max=pca_comp_max)\n",
    "\n",
    "        clusters = {k: [answers_by_qid[qid][idx] for idx in v ] for k, v in clusters.items()}\n",
    "        clusters_by_qid[qid] = clusters\n",
    "\n",
    "    return clusters_by_qid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "def get_random_clusters(questions, annotations):\n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters = defaultdict(lambda: defaultdict(list))\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        n_clusters = np.random.choice(range(1, len(list_of_qas)))\n",
    "        for quest, ann in list_of_qas:\n",
    "            clust_idx = np.random.choice(range(n_clusters))\n",
    "            question = quest['question']\n",
    "            clust_data = {\"id\": ann['answers'][0]['mturk_id'], \"answer\": ann['answers'][0]['answer']}\n",
    "            clusters[qid][str(clust_idx)].append(clust_data)\n",
    "        \n",
    "    return clusters  \n",
    "\n",
    "def get_baseline_clusters(questions, annotations, full_recall):\n",
    "    anns_by_qid = defaultdict(list)\n",
    "    for quest, ann in zip(questions, annotations):\n",
    "        qid, i = quest['question_id'].split(\"_\")\n",
    "        anns_by_qid[qid].append((quest, ann))\n",
    "\n",
    "    clusters = defaultdict(lambda: defaultdict(list))\n",
    "    for qid, list_of_qas in anns_by_qid.items():\n",
    "        if full_recall: \n",
    "            # one single cluster\n",
    "            for i, (quest, ann) in enumerate(list_of_qas):\n",
    "                clust_data = {\"id\": ann['answers'][0]['mturk_id'], \"answer\": ann['answers'][0]['answer']}\n",
    "                clusters[qid][0].append(clust_data)\n",
    "        else:\n",
    "            # one cluster per annotation\n",
    "            for i, (quest, ann) in enumerate(list_of_qas):\n",
    "                clust_data = {\"id\": ann['answers'][0]['mturk_id'], \"answer\": ann['answers'][0]['answer']}\n",
    "                clusters[qid][str(i)].append(clust_data)\n",
    "    return clusters \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "curr_path = pathlib.Path('').resolve().parent\n",
    "\n",
    "sys.path.insert(0, str(curr_path.joinpath(\"hit3.0\").joinpath(\"results\")))\n",
    "from process_csv import f1_score\n",
    "\n",
    "def preprocess(cluster_data):\n",
    "    if type(cluster_data) in [dict, defaultdict]:\n",
    "        # dealing with predicted clusters or preprocessed clusters\n",
    "        return cluster_data.values()\n",
    "    return cluster_data\n",
    "\n",
    "def get_scores(clusters_by_qid_a, clusters_by_qid_b):\n",
    "    scores = []\n",
    "    for qid in clusters_by_qid_a.keys():\n",
    "        cluster_a = preprocess(clusters_by_qid_a[qid])\n",
    "        cluster_b = preprocess(clusters_by_qid_b[qid])\n",
    "        f1_tuple = f1_score(cluster_a, cluster_b)\n",
    "        f1_tuple = f1_tuple[0:-1]\n",
    "        scores.append(f1_tuple)\n",
    "    # print(scores)\n",
    "    scores = np.array(scores)\n",
    "    return np.mean(scores, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_clusters = get_annotator_clusters(questions, annotations)\n",
    "# print(json.dumps(ann_clusters, indent=4))\n",
    "\n",
    "glove_clusters = get_preprocessed_clusters(questions, annotations)\n",
    "# print(json.dumps(glove_clusters, indent=4))\n",
    "\n",
    "random_clusters = get_random_clusters(questions, annotations)\n",
    "# print(json.dumps(random_clusters, indent=4))\n",
    "\n",
    "full_recall_clusters = get_baseline_clusters(questions, annotations, full_recall=True)\n",
    "# print(json.dumps(random_clusters, indent=4))\n",
    "\n",
    "full_precision_clusters = get_baseline_clusters(questions, annotations, full_recall=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOVE: P: 97.80, R: 63.73, F1: 71.61\n",
      "RANDOM: P: 72.08, R: 71.49, F1: 63.62\n",
      "RECALL: P: 63.61, R: 100.00, F1: 76.45\n",
      "PRECISION: P: 100.00, R: 50.73, F1: 61.19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "glove_to_ann = get_scores(glove_clusters, ann_clusters)\n",
    "\n",
    "print(f\"GLOVE: P: {glove_to_ann[1]*100:.2f}, R: {glove_to_ann[2]*100:.2f}, F1: {glove_to_ann[0]*100:.2f}\")\n",
    "\n",
    "random_to_ann = get_scores(random_clusters, ann_clusters)\n",
    "\n",
    "print(f\"RANDOM: P: {random_to_ann[1]*100:.2f}, R: {random_to_ann[2]*100:.2f}, F1: {random_to_ann[0]*100:.2f}\")\n",
    "\n",
    "recall_to_ann = get_scores(full_recall_clusters, ann_clusters)\n",
    "print(f\"RECALL: P: {recall_to_ann[1]*100:.2f}, R: {recall_to_ann[2]*100:.2f}, F1: {recall_to_ann[0]*100:.2f}\")\n",
    "\n",
    "precision_to_ann = get_scores(full_precision_clusters, ann_clusters)\n",
    "print(f\"PRECISION: P: {precision_to_ann[1]*100:.2f}, R: {precision_to_ann[2]*100:.2f}, F1: {precision_to_ann[0]*100:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a71203a0affc7207a6dfd2214cf786ae35ec96fb0be57392a936686760a696bf"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
