<!-- You must include this JavaScript file -->
<script src="https://assets.crowd.aws/crowd-html-elements.js"></script>

<!-- For the full list of available Crowd HTML Elements and their input/output documentation,
      please refer to https://docs.aws.amazon.com/sagemaker/latest/dg/sms-ui-template-reference.html -->

<!-- You must include crowd-form so that your task submits answers to MTurk -->
<crowd-form answer-format="flatten-objects">

    <!-- The crowd-classifier element will create a tool for the Worker to select the
           correct answer to your question.

          Your image file URLs will be substituted for the "image_url" variable below 
          when you publish a batch with a CSV input file containing multiple image file URLs.
          To preview the element with an example image, try setting the src attribute to
          "https://s3.amazonaws.com/cv-demo-images/one-bird.jpg" -->
          
    <div>
                <p><strong>Question: ${question}</p>
                <p><strong>Answers: ${answers}</p>
    </div>
    <crowd-image-classifier 
        src="${image_url}"
        categories="['No Disagreement', 'Subjective question', 'Mental state', 'Inferring motion', 'Predicting future', 'Specificity difference', 'Bad image', 'Nonsensical question', 'None of the Above']"
        header="  "
        name="category">
        
       <!-- Use the short-instructions section for quick instructions that the Worker
              will see while working on the task. Including some basic examples of 
              good and bad answers here can help get good results. You can include 
              any HTML here. -->
        <short-instructions>
            <p>Examine the question and answers with the image.</p>
            <p>Choose the reason why people might have given different answers.</p>
        </short-instructions>

        <!-- Use the full-instructions section for more detailed instructions that the 
              Worker can open while working on the task. Including more detailed 
              instructions and additional examples of good and bad answers here can
              help get good results. You can include any HTML here. -->
        <full-instructions header="Classification Instructions">
            <p>You will be presented with a series of images, questions about those images, and candidate answers.
            <p>The answers were obtained from real people, who sometimes disagree on the answer to an image-question pair. The goal of this
HIT is to annotate reasons why people disagree on an example.</p>
            <p></p>
            <p>Please read all the instructions before beginning. There will be two tasks for each image-question-answer example presented.</p>
            <p>The first task is to determine whether the candidate answers fall into multiple distinct groups.</p>
        
            <p>For example, for the following example, there are 2 distinct groups:</p>
            <p><b>Question:</b> "Are they the kid's skis?"</p>
            <p><b>Answers:</b> yes, no, yes, no, no, yes </p>
            <img src="">
            
            <p>On the other hand, this example does not have distinct groups, even though the responses are slightly different.
            
            <p><b>Question:</b> "What is the woman in the room doing?"</p>
            <p><b>Answers:</b> talking to someone, talking, talking, talking to someone outside of window, talking, talking out window </p>
            <img src="">
            
            <p>If there are distinct groups, the second task is the categorize the examples by why you think there are different groups.</p>
            
            <p>Possible reasons are:
            <ol>
                <li><b>Subjective questions</b> Some questions will ask for subjective responses, people might reasonably disagree. 
                For example: 
                    <p><b>Question:</b> "Has the wall been painted recently"</p>
                    <p><b>Answers:</b> yes, no, no, yes, no, yes </p>
                    <p><img src="https://esteng.github.io/misc/uncert/instructions/recently.jpg"></p>
                </li>
                <li><b>Inferring motion</b> Some questions will ask about motion, which may be hard to infer from an image. Example: 
                    <p><b>Question:</b> "Is the skateboarder moving?"</p>
                    <p><b>Answers:</b> yes,no,yes,no,no </p>
                    <p><img src="https://esteng.github.io/misc/uncert/instructions/skateboarder.jpg"></p>
                </li>
                <li><b>Questions about mental states</b>: Questions like "Is the man happy?" or "Do the elephants like getting wet?" can cause disagreements. These are similar to subjective questions, but are specifically about mental states like happiness, sadness, and preference. </li>
                <li><b>Questions about predicting the future:</b> Some questions ask annotators to predict the future. Example: 
                    <p><b>Question:</b> "If the man steps forward, will he fall onto the track?"</p>
                    <p><b>Answers:</b> yes, no, yes, yes, no, no</p>
                    <p><img src="https://esteng.github.io/misc/uncert/instructions/falling.jpg"></p>
                </li>
                <li><b>Differences in answer specificity:</b> For these questions, some annotators will choose to answer at different levels of specificy. For example, in this question, some annotators answered the question literally (they can identify the job) while others answered the implied question (what job is it). Even those who answered gave different levels of specificy (flagger vs traffic control):
                    <p><b>Question:</b> "Can you identify this job?" </p>
                    <p><b>Answers:</b> traffic control, yes, yes, flagger</p>
                    <p><img src="https://esteng.github.io/misc/uncert/instructions/flagger.jpg"></p>
                </li>
            </ol>
            
            <p>Other options are:</p>
            <ol>
                <li><b>No disagreement:</b> There is no actual disagreement between annotators.
                <li><b>Poor image quality:</b> The image quality is low, making the question difficult to answer.</li>
                <li><b>Nonsensical question:</b> The question does not make sense, or isn't about the image.</li>
                </li>
            </ol>
            <p>If the image falls into <b>none</b> of these categories, please check the 'None of the above' option, and specify in as few words as possible what is causing the annotator disagreement in the text box. Otherwise, please leave the text box blank. </p>
        </full-instructions>
    </crowd-image-classifier>
    <p><crowd-input name="other" placeholder="Other" ></crowd-input></p>

    
</crowd-form>