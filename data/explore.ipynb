{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json \n",
    "import pathlib\n",
    "import Levenshtein as lev\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "data_path = pathlib.Path(\"/srv/local2/estengel/uncertainty_data/vqa/\")\n",
    "\n",
    "val_ann_path = data_path.joinpath(\"v2_mscoco_val2014_annotations.json\")\n",
    "val_quest_path = data_path.joinpath(\"v2_OpenEnded_mscoco_val2014_questions.json\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(val_ann_path) as f1, open(val_quest_path) as f2:\n",
    "    val_anns = json.load(f1)\n",
    "    val_quests = json.load(f2)\n",
    "\n",
    "\n",
    "\n",
    "question_lookup = {e['question_id']: e for e in val_quests['questions']}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np \n",
    "import string \n",
    "from tqdm import tqdm \n",
    "\n",
    "def charify(s1, s2):\n",
    "    \"\"\"\n",
    "    Levenshtein lib operates over strings of characters, rather than lists of strings\n",
    "    It's really fast so we want to use it. This is a slightly hacky fix to do that.\n",
    "    Given 2 lists of strings, get a joint vocab over them, then map each item in the vocab\n",
    "    to a unique char. Replace the strings with chars, then concatenate. \n",
    "    \"\"\"\n",
    "    total_vocab = set(s1) | set(s2)\n",
    "    chars = string.ascii_letters + string.digits + string.punctuation\n",
    "    try:\n",
    "        assert(len(total_vocab) < len(chars))\n",
    "    except AssertionError:\n",
    "        print(\"Warning: mapping incomplete, returning large distance to ignore in min\")\n",
    "        return np.inf\n",
    "    chars = chars[0:len(total_vocab)]\n",
    "    total_vocab = list(total_vocab)\n",
    "    mapping = {k:c for k, c in zip(total_vocab, chars)}\n",
    "\n",
    "    s1 = [mapping[x] for x in s1]\n",
    "    s2 = [mapping[x] for x in s2]\n",
    "    s1 = \"\".join(s1)\n",
    "    s2 = \"\".join(s2)\n",
    "    return (s1, s2, mapping)\n",
    "\n",
    "def get_pairwise_lev(ann_set):\n",
    "\n",
    "    scores = np.ones((len(ann_set), len(ann_set))) * np.inf\n",
    "    for i, ann0 in enumerate(ann_set):\n",
    "        for j, ann1 in enumerate(ann_set): \n",
    "            if i == j:\n",
    "                scores[i,j] = 0.0\n",
    "                continue\n",
    "            c_ann0, c_ann1, mapping = charify(ann0.split(\" \"), ann1.split(\" \"))\n",
    "\n",
    "            distance = lev.distance(c_ann0, c_ann1)\n",
    "            scores[i,j] = distance\n",
    "    return scores \n",
    "\n",
    "c = 0\n",
    "all_dists_and_idxs = []\n",
    "for i, annotation in tqdm(enumerate(val_anns['annotations'])):\n",
    "\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "    pairwise_distances = get_pairwise_lev(annotation_set)\n",
    "    # print([x['answer'] for x in annotation_set['answers']])\n",
    "    mean_distance = np.mean(pairwise_distances, axis=0)\n",
    "    mean_mean_distance = np.mean(mean_distance, axis=0)\n",
    "\n",
    "    all_dists_and_idxs.append((i, mean_mean_distance))\n",
    "\n",
    "    #if mean_mean_distance > 3:\n",
    "    #    question = question_lookup[annotation['question_id']]\n",
    "    #    print(question['question'])\n",
    "    #    print(annotation_set)\n",
    "    #    print(pairwise_distances)\n",
    "    #    print(mean_mean_distance)\n",
    "    #    c +=1 \n",
    "        #if c > 10:\n",
    "        #    break\n",
    "\n",
    "\n",
    "\n",
    "sorted_dists = sorted(all_dists_and_idxs, key = lambda x:x[1])\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VQA Notes\n",
    "The above code tries to identify high-disagreement examples by measuring the pairwise Levenshtein distance between each annotation, and isolating ones with a high distance. \n",
    "\n",
    "### Preliminary observations\n",
    "High-distance pairs fall into a few groups: \n",
    "1. **color questions**:\n",
    "    Questions like \"What color is X\" (where X is multi-colored). Annotators choose different colors. Example:\n",
    "    ```What color is the plane?\n",
    "    image id: 524011\n",
    "    answers: ['blue', 'blue white and red', 'blue', 'light blue, dark blue, white, and red', 'blue white', \n",
    "            'blue', 'white, blue and red', 'blue', 'white and light blue', 'baby blue and white']```\n",
    "    These questions aren't great, there isn't actually a lot of disagreement here, just lexical variation \n",
    "2. **text from a sign**: \n",
    "    Questions like \"What does the sign say\", where annotators vary in what they say/how they order it. Example:\n",
    "    ```What do the road signs say?\n",
    "       image id: 261273\n",
    "       answers: ['stop, regulus st, stampede rd', 'stop', 'stampede, regulus, stop', 'stop', \n",
    "                 'stop, stampede, and regulus', 'regulus st, stampede rd', 'regulus st, stampede rd, and stop', \n",
    "                 'regulus stampede stop', 'stop']\n",
    "    ``` \n",
    "    Example:\n",
    "    ```\n",
    "    What kinds of foods are available according to the sign?\n",
    "    image id: 331688\n",
    "    answers: ['italian', '4', 'sausage, hamburgers, hot dogs', 'meat', 'hamburgers, and hot dogs', \n",
    "              'italian sausage and beef, hamburgers, hot dogs', 'italian sausage combos, italian beef, hamburgers, hot dogs', \n",
    "              'fast food', 'meat']\n",
    "    ```\n",
    "\n",
    "3. **Underspecified referrents**:\n",
    "    Some questions have multiple correct answers because the question makes a unique reference to a non-unique item (e.g. \"what does this sign say\" when there are 3 signs in the image, annotators choose differents signs to answer)\n",
    "\n",
    "    Example: \n",
    "    ````\n",
    "    What is this sign mean?\n",
    "    image id: 331688\n",
    "    answers: ['food available', '2 way traffic', '2 way street', 'it is restaurant menu', 'traffic goes both ways through construction']\n",
    "    ```\n",
    "\n",
    "4. **Why questions**: \n",
    "    Some questions ask for why something is, rather than what/how. Arguably, these are more commonsense inference questions than anything, and really don't require visual reasoning as much as they require world knowledge. Annotators give varying explanations. Example: \n",
    "\n",
    "    ```\n",
    "    Why is there a stop sign on the bus towards the rear?\n",
    "    image id: 130352\n",
    "    answers: ['school bus', 'warning for others', 'to show people behind when not to pass bus', 'for cars to stop', 'yes', 'stop traffic for bus passenger', 'stop traffic', 'cars', 'for when kids get on']\n",
    "    ```\n",
    "\n",
    "    Example: \n",
    "    ```\n",
    "    Why is the person stopped?\n",
    "    image id: 261888\n",
    "    answers: ['waiting', 'waiting for animals to pass', 'look at horses', \n",
    "              'to watch 2 horses in other lane', 'looking at horses', 'watching livestock', 'looking at something']\n",
    "    ```\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# best = sorted_dists[0:10]\n",
    "worst = sorted_dists[-20:]\n",
    "\n",
    "for w, dist in worst:\n",
    "    annotation = val_anns['annotations'][w]\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "    question = question_lookup[annotation['question_id']]\n",
    "    print(question['question'])\n",
    "    print(f\"image id: {annotation['image_id']}\")\n",
    "    print(annotation_set)\n",
    "    print(dist)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What about Glove\n",
    "Levenshtein distance wasn't any good, how about using the mean glove embedding "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np \n",
    "import sys \n",
    "import pdb \n",
    "import re \n",
    "\n",
    "def get_glove(path):\n",
    "    lookup = {}\n",
    "    with open(path) as f1:\n",
    "        for line in f1:\n",
    "            splitline = re.split(\"\\s+\", line.strip())\n",
    "            word = splitline[0]\n",
    "            try:\n",
    "                vec = np.array(splitline[1:], dtype=float) \n",
    "            except ValueError:\n",
    "                continue\n",
    "            lookup[word] = vec\n",
    "    return lookup \n",
    "glove_lookup = get_glove(\"/srv/local1/estengel/resources/glove.840B.300d.txt\")\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "np.random.seed(12)\n",
    "\n",
    "def get_glove_embedding(word):\n",
    "    return glove_lookup[word]\n",
    "\n",
    "\n",
    "def get_pairwise_glove(annotation_set):\n",
    "    annotation_set_as_vec = []\n",
    "    for i, ann in enumerate(annotation_set):\n",
    "        ann = ann.split(\" \")\n",
    "        ann = [w for w in ann if w in glove_lookup.keys()]\n",
    "        if len(ann) == 0:\n",
    "            continue\n",
    "\n",
    "        vecs = np.array([get_glove_embedding(word) for word in ann])\n",
    "        annotation_set_as_vec.append(np.mean(vecs, axis=0) )\n",
    "\n",
    "    if len(annotation_set) == 0 or len(annotation_set_as_vec) == 0:\n",
    "        return np.zeros((1,1))\n",
    "\n",
    "    annotation_set_as_vec = np.array(annotation_set_as_vec).reshape(-1, 300)\n",
    "    # similarity_matrix = np.dot(annotation_set_as_vec.T, annotation_set_as_vec) / 2 * np.sum(annotation_set_as_vec, axis=0, keepdims=True)\n",
    "    similarity_matrix = cosine_similarity(annotation_set_as_vec) \n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "glove_all_dists_and_idxs = []\n",
    "for i, annotation in tqdm(enumerate(val_anns['annotations'])):\n",
    "\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "    pairwise_similarity = get_pairwise_glove(annotation_set)\n",
    "    mean_similarity = np.mean(pairwise_similarity, axis=0)\n",
    "    mean_mean_similarity = np.mean(mean_similarity, axis=0)\n",
    "    # print(pairwise_similarity)\n",
    "    # print(mean_mean_similarity)\n",
    "    glove_all_dists_and_idxs.append((i, mean_mean_similarity))\n",
    "    # sys.exit()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import seaborn as sns\n",
    "\n",
    "values = [x[1] for x in glove_all_dists_and_idxs]\n",
    "sns.histplot(values)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "# ignore numeric \n",
    "def is_not_numeric(idx):\n",
    "    annotation = val_anns['annotations'][idx]\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "    for ann in annotation_set:\n",
    "        words = ann.split(\" \")\n",
    "        for w in words:\n",
    "            if re.match(\"[0-9]+\", w) is not None:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# filter by length \n",
    "def is_n_long(idx, min_len=5):\n",
    "    annotation = val_anns['annotations'][idx]\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "\n",
    "    if len(annotation_set) < min_len:\n",
    "        return False\n",
    "    return True \n",
    "\n",
    "glove_non_zero = [(i,x) for i,x in glove_all_dists_and_idxs if x > 0.0]\n",
    "glove_no_nums = [(i,x) for i,x in glove_non_zero if is_not_numeric(i)]\n",
    "removed_no_nums = len(glove_all_dists_and_idxs) - len(glove_no_nums)\n",
    "print(f\"removed {removed_no_nums} for numbers\")\n",
    "glove_min_len = [(i,x) for i,x in glove_no_nums if is_n_long(i)]\n",
    "removed_min_len = len(glove_no_nums) - len(glove_min_len)\n",
    "print(f\"removed {removed_min_len} for being too short\")\n",
    "print(f\"remaining: {len(glove_min_len)}\")\n",
    "\n",
    "sorted_dists = sorted(glove_min_len, key = lambda x:x[1])\n",
    "worst = sorted_dists[0:20]\n",
    "\n",
    "for w, dist in worst:\n",
    "    annotation = val_anns['annotations'][w]\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "    question = question_lookup[annotation['question_id']]\n",
    "    print(question['question'])\n",
    "    print(f\"image id: {annotation['image_id']}\")\n",
    "    print(annotation_set)\n",
    "    print(dist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# What about glove\n",
    "These look a lot better, but the current method gets the maximally different examples, where each annotator basically says something different. It looks like most of these examples are due to underspecification either in the image or question.\n",
    "\n",
    "Example: Image has multiple posters/boxes with 2nd lines, so each annotator is right but has picked a different object. \n",
    "```Question: What are the words on the second line?\n",
    "image id: 554838\n",
    "Answers: ['magnum', 'hell', 'almendras', 'almendras', 'yes', 'corn flakes', 'magnum']```\n",
    "\n",
    "In other examples, annotators pick different attributes to focus on. \n",
    "\n",
    "Example: (picture of an old RC Delorean) \n",
    "```What kind of car is that?\n",
    "image id: 16838\n",
    "['delorean', 'older one', 'lotus', 'toy', 'remote control model', 'delorean']\n",
    "```\n",
    "Some care that it's an old car, some car that it's RC, some care that it's a Delorean. \n",
    "\n",
    "## Other improvements\n",
    "What about K-means to get fixed number of clusters, then order by most coherent clusters. That way I can say, \"Give me the examples that have K distinct answers of clusters, with annotators going one of two ways\" and set K to 2 or 3\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "K = 2\n",
    "kmeans_wrapper = KMeans(n_clusters=K, random_state=12)\n",
    "\n",
    "def get_kmeans_glove(annotation_set):\n",
    "    annotation_set_as_vec = []\n",
    "    for i, ann in enumerate(annotation_set):\n",
    "        ann = ann.split(\" \")\n",
    "        ann = [w for w in ann if w in glove_lookup.keys()]\n",
    "        if len(ann) == 0:\n",
    "            continue\n",
    "\n",
    "        vecs = np.array([get_glove_embedding(word) for word in ann])\n",
    "        annotation_set_as_vec.append(np.mean(vecs, axis=0) )\n",
    "\n",
    "    if len(annotation_set) == 0 or len(annotation_set_as_vec) < 3:\n",
    "        return np.inf, [], -1\n",
    "\n",
    "    annotation_set_as_vec = np.array(annotation_set_as_vec).reshape(-1, 300)\n",
    "    # run kmeans\n",
    "    kmeans = kmeans_wrapper.fit(annotation_set_as_vec) \n",
    "    # intertia is sum of squared distances of samples to their closest cluster center \n",
    "    # high intertia means dispersed, low means fits well to the number of clusters\n",
    "    centers = kmeans.predict(annotation_set_as_vec) \n",
    "    # score based on how balanced centers is \n",
    "    balance_center = 0.5\n",
    "    center_mean = np.mean(centers)\n",
    "    print(f\"center mean {center_mean}\")\n",
    "    center_score = -2*np.abs(center_mean - balance_center) + 1\n",
    "    return kmeans.inertia_, kmeans.predict(annotation_set_as_vec), center_score\n",
    "\n",
    "\n",
    "kmeans_dists_and_idxs = []\n",
    "for i, annotation in tqdm(enumerate(val_anns['annotations'])):\n",
    "\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "    kmeans_distance, kmeans_centers, center_score = get_kmeans_glove(annotation_set)\n",
    "    kmeans_dists_and_idxs.append((i, (1-center_score) * kmeans_distance))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sorted_dists = sorted(kmeans_dists_and_idxs, key = lambda x:x[1])\n",
    "worst = sorted_dists[0:20]\n",
    "\n",
    "for w, dist in worst:\n",
    "    annotation = val_anns['annotations'][w]\n",
    "    annotation_set = [x['answer'] for x in annotation['answers'] if x['answer_confidence'] == 'yes']\n",
    "    question = question_lookup[annotation['question_id']]\n",
    "    print(question['question'])\n",
    "    print(f\"image id: {annotation['image_id']}\")\n",
    "    print(annotation_set)\n",
    "    print(dist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('cert': conda)"
  },
  "interpreter": {
   "hash": "875f2f4ff9dafdf3a088249020f95864ee5ef190841b012fbbeb50c022de1513"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}